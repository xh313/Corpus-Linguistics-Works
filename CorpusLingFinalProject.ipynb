{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CorpusLingFinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2p_z0LNEVIJ5"
      ],
      "authorship_tag": "ABX9TyMX1eKlCEHxKpIhLloV0I0d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xh313/Corpus-Linguistics-Works/blob/main/CorpusLingFinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Planning"
      ],
      "metadata": {
        "id": "8rF6plh7ycKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspiration:\n",
        "![meme](https://user-images.githubusercontent.com/77285010/161146641-d099beed-56d7-40ef-8ae4-0083fe473d6f.png)\n",
        "\n",
        "(source: https://www.reddit.com/r/Suomi/comments/tqx5ac/mielipide_suomesta_eri_alaredditeiss%C3%A4/)\n",
        "\n",
        "When I was learning Finnish through browsing the Finnish language community on reddit r/suomi (meaning Finland in Finnish), I saw this meme which makes a contrast between the English language subreddit on Finland and the native language one. Obviously, the contrast between the sentiment on the place of the natives and the 'tourists' is pretty hilarious. I am thus intrigued to see what the differences are through analysing the corpora!\n"
      ],
      "metadata": {
        "id": "k596vr7xuvRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Idea\n",
        "\n",
        "- Scrap the English and native language subreddits of different places in the world and compare sentiments.\n",
        "- Assume that the English corpora represent the discussions within 'international community', while the native corpora represent the 'local community'.\n",
        "- Compare and contrast:\n",
        "  - English vs Native sentiments\n",
        "  - How the contrast is different for different countries / places around the world."
      ],
      "metadata": {
        "id": "a8wNNVUVv5KU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods\n",
        "- Scrap the subreddits\n",
        "- Translation: first compare if the sentiment of the translated corpora is similar enough to the original corpora by sampling a few languages\n",
        "- If translation does not strip the sentiment too much, the translated corpora would be used to do comparative analysis."
      ],
      "metadata": {
        "id": "KQWpV0-yxpNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "[NOT UPDATING] Channel lists:\n",
        "\n",
        "No longer updating since this is too laborious.\n",
        "\n",
        "E = English, N = Native\n",
        "\n",
        "- Poland:\n",
        "  - E: https://www.reddit.com/r/poland/\n",
        "  - N: https://www.reddit.com/r/Polska/\n",
        "\n",
        "- Finland:\n",
        "  - E: https://www.reddit.com/r/Finland/\n",
        "  - N: https://www.reddit.com/r/Suomi/\n",
        "\n",
        "- Brazil:\n",
        "  - E: https://www.reddit.com/r/Brazil/\n",
        "  - N: https://www.reddit.com/r/brasil/\n",
        "\n",
        "- Turkey:\n",
        "  - E: https://www.reddit.com/r/Turkey/\n",
        "  - N: https://www.reddit.com/r/Turkiye/"
      ],
      "metadata": {
        "id": "62IIcmhs2jWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Channel Lists (New):"
      ],
      "metadata": {
        "id": "mWL00b3qMes3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester list to run faster\n",
        "test_srs = ['Finland', 'Suomi', \n",
        "       'Brazil', 'brasil'\n",
        "       ]\n",
        "\n",
        "# Tester list for translation analysis\n",
        "trans_srs = [#'poland', 'Polska',   # The Poland sr does not work properly\n",
        "       'Finland', 'Suomi', \n",
        "       'Brazil', 'brasil',\n",
        "       #'Turkey', 'Turkiye',\n",
        "       'China', 'China_irl',  # Uncertain if this counts\n",
        "       #'japan', 'ja',\n",
        "       #'Thailand', 'thaithai',\n",
        "       #'spain', 'es',  # The 'spain' one is flooded w spanish as well...\n",
        "       #'southafrica', 'RSA',\n",
        "       ]\n",
        "\n",
        "# Current working list\n",
        "# English, Native format!!\n",
        "srs = ['poland', 'Polska',   # The Poland sr does not work properly\n",
        "       'Finland', 'Suomi', \n",
        "       'Brazil', 'brasil',\n",
        "       'Turkey', 'Turkiye',\n",
        "       'China', 'China_irl',  # Uncertain if this counts\n",
        "       'japan', 'ja',\n",
        "       'Thailand', 'thaithai',\n",
        "       'spain', 'es',  # The 'spain' one is flooded w spanish as well...\n",
        "       'southafrica', 'RSA',\n",
        "       'Norway', 'norge',\n",
        "       ]"
      ],
      "metadata": {
        "id": "ImvYf0MSCW4B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Always run the list cell after adding new subreddits to make sure that the list is up-to-date"
      ],
      "metadata": {
        "id": "GWZcpX4fMsiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Issues / Limitations"
      ],
      "metadata": {
        "id": "KPVf9O__yFSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To-be-resolved list\n",
        "\n",
        "1. Some native language forums have bits and pieces of English mixed in, or just reddit codes and stuff in English.\n",
        "2. How many hot posts should I extract per forum?\n",
        "3. Naïve Bayes only telling the positive / negative or tone analysis instead?\n",
        "4. Not-so-accurate results for highly-infleceted langs?"
      ],
      "metadata": {
        "id": "layPPId-yL-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unresolved limitations\n",
        "\n",
        "1. Size of subreddits vary a lot\n",
        "2. Naturally the topic focus would be subtly different in different subreddits, which ones are comparable?"
      ],
      "metadata": {
        "id": "kyjzv9jWyUY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codes"
      ],
      "metadata": {
        "id": "FtyDqvwsyjB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations / Mounting / Importing"
      ],
      "metadata": {
        "id": "xdUWuwNY41vg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installations"
      ],
      "metadata": {
        "id": "sKicHvp15EPy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiUo8q5Iupna",
        "outputId": "b5cc9401-a247-41d9-ad73-626f09200f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.5.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.54.0\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 807 kB/s \n",
            "\u001b[?25hCollecting prawcore<3,>=2.1\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting update-checker>=0.18\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
            "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
            "Successfully installed praw-7.5.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install praw"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing"
      ],
      "metadata": {
        "id": "Z_N54e_g5HBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "\n",
        "reddit = praw.Reddit(client_id=\"R7keGb8iIXeFuLo-CZOs7g\",\n",
        "                     client_secret=\"ieKEfzD3VvRdmg-HGR0yUt3A6jD_Lw\",\n",
        "                     user_agent=\"scrapata\",\n",
        "                     username='hxhthewebscraper',\n",
        "                     password='Puhun_suomea4')\n",
        "\n",
        "print(reddit.user.me())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpfhZPFQ5J7S",
        "outputId": "61cb8081-b5ca-4386-c503-024b9e8ef6ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hxhthewebscraper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "SKnFVzyUHkC1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting"
      ],
      "metadata": {
        "id": "NPT--q3OHbZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iopko5FHHdy8",
        "outputId": "09a158ae-a6da-4583-9210-903d75b54db0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping"
      ],
      "metadata": {
        "id": "aGgGlanM5Rli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subr_to_list(subreddit, number):\n",
        "  \"\"\"\n",
        "  Takes a subreddit name and scraps posts into a list.\n",
        "  Returns the list.\n",
        "  \"\"\"\n",
        "  subr_list = list() #key is body, value is judgement\n",
        "\n",
        "  # get 10 hot posts from the r/Suomi subreddit\n",
        "  hot_posts = reddit.subreddit(subreddit).hot(limit=number) #this is pulling out the hot posts\n",
        "  for post in hot_posts:  \n",
        "      submission = reddit.submission(id=post.id)\n",
        "      subr_list.append(submission.selftext)\n",
        "      #linked_post = reddit.submission(url='https://www.reddit/com/'+post.url)\n",
        "      #for top_level_comment in submission.comments:\n",
        "        #print(top_level_comment.body)\n",
        "        #subr_dict[submission.selftext] = top_level_comment.body\n",
        "\n",
        "  return subr_list"
      ],
      "metadata": {
        "id": "r3n47ufx5QNz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_txt(postlist, filename, ordinal='1'):\n",
        "  \"\"\"\n",
        "  Takes a list of processed subreddit posts and output a .txt file.\n",
        "  The ordinal in the end is added to sort the files properly.\n",
        "  \"\"\"\n",
        "  outfile = open('/content/drive/My Drive/LGCS124/FinalProject/TestCorpora/' + str(int(ordinal)) + '_' + filename + '.txt', 'w')\n",
        "\n",
        "  for post in postlist:\n",
        "    outfile.write(post)\n",
        "    outfile.write('\\n')\n",
        "\n",
        "  outfile.close()"
      ],
      "metadata": {
        "id": "TtTzIo3i8_I6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester: [DON'T RERUN THIS IT TAKES AGES!!!]\n",
        "\n",
        "#finland_e = subr_to_list('Suomi', 100)"
      ],
      "metadata": {
        "id": "LWwphQBu6kKN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list_to_txt(finland_e, 'finland_e')"
      ],
      "metadata": {
        "id": "pWXKAQwx-wI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The ultimate scraper!"
      ],
      "metadata": {
        "id": "dSzYQ9ebBuhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subr_extract(subreddit, number, ordinal='1'):\n",
        "  \"\"\"\n",
        "  The ultimate extraction. Takes a string that is the name of the subreddit\n",
        "  and the number of posts to be scraped.\n",
        "  The ordinal in the end is added to sort the files properly.\n",
        "  \"\"\"\n",
        "  postlist = subr_to_list(subreddit, number)\n",
        "  list_to_txt(postlist, subreddit, ordinal)"
      ],
      "metadata": {
        "id": "syU0rKkC_vbm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester\n",
        "subr_extract('Polska', 10)"
      ],
      "metadata": {
        "id": "O1i0cPXMBSaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester\n",
        "ord = 1\n",
        "for sr in test_srs:\n",
        "  subr_extract(sr, 10, ord)\n",
        "  ord += 0.5"
      ],
      "metadata": {
        "id": "-Rl5dWY-C1Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function written to scrape the list [srs](https://colab.research.google.com/drive/1Fh5wP9f8Qt47cjE8xgT86ub9eWUZo-iR#scrollTo=ImvYf0MSCW4B&line=3&uniqifier=1)"
      ],
      "metadata": {
        "id": "eRl5cvzIGPjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrap_srs(currentsrs, number):\n",
        "  ord = 1\n",
        "  for sr in currentsrs:\n",
        "    subr_extract(sr, number, ord)\n",
        "    ord += 0.5"
      ],
      "metadata": {
        "id": "-8i-wlBBGrwF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call this to get all files from current srs into the drive\n"
      ],
      "metadata": {
        "id": "8w09pkOeIa5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DON'T RUN UNLESS YOU WANT TO GET THE FILE NOW!\n",
        "#scrap_srs(srs, 30)"
      ],
      "metadata": {
        "id": "2G1vILroIiY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation testing\n",
        "This is to check if the sentiment would be lost after translation of the non-English corpora into English. Two translators would be used:\n",
        "- DeepL\n",
        "- Google Translate\n",
        "\n",
        "To test translation, three pairs of corpora are used:\n",
        "1. Finland (Finnish, English)\n",
        "2. Brazil (Brazilian Portuguese, English)\n",
        "3. China (Simplified Chinese, English)\n",
        "\n",
        "Of which Finnish has the highest morphological complexity being an agglutinative language, Brazilian Portuguese the medium and Chinese the lowest. "
      ],
      "metadata": {
        "id": "kTH6wtSVkimp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code cell to get translation tester corpora\n",
        "scrap_srs(trans_srs, 30)"
      ],
      "metadata": {
        "id": "OX-MD1Imk6qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Finnish sentiment analysis, the testing corpus used is from the Korp API: https://korp.csc.fi/. I referenced https://github.com/tjkemp/ubik-sentiment for the code used to strip data from the corpus. It is combined with another labeled corpus from Turku University."
      ],
      "metadata": {
        "id": "LOikpMifxibk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Corpus input"
      ],
      "metadata": {
        "id": "4S3E3vfPPdVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Turku corpus"
      ],
      "metadata": {
        "id": "HwCvKmjoVF3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/LGCS124/FinalProject/TranslationTrainingData/FinnSentiment2020.tsv', sep='\\t', header=0)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IMoBrIjjPcrt",
        "outputId": "78b52376-6710-4b90-a4fa-8be7608c0a5b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   a  b  c  major  derive  smiley  produ  split  batch  \\\n",
              "0  1  0  1      1       4       0     -1      1      1   \n",
              "1  0  1  0      0       4       0     -1     12      1   \n",
              "2  0  0  0      0       3       0     -1     14      1   \n",
              "3  1  1  1      1       5       0      1      7      1   \n",
              "4  1  1  1      1       5       0      1     12      1   \n",
              "\n",
              "           index in original  \\\n",
              "0  comments2008c.vrt 2145269   \n",
              "1  comments2011c.vrt 3247745   \n",
              "2  comments2007c.vrt 3792960   \n",
              "3  comments2010d.vrt 2351708   \n",
              "4  comments2007d.vrt 1701675   \n",
              "\n",
              "                                                text  \n",
              "0                        - Tervetuloa skotlantiin...  \n",
              "1  ...... No, oikein sopiva sattumaha se vaan oli...  \n",
              "2                                                40.  \n",
              "3             Kyseessä voi olla loppuelämäsi nainen.  \n",
              "4                 Sinne vaan ocean clubiin iskemään!  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-675ea4f9-2d68-4bff-96ae-32c6a0dd00bf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>major</th>\n",
              "      <th>derive</th>\n",
              "      <th>smiley</th>\n",
              "      <th>produ</th>\n",
              "      <th>split</th>\n",
              "      <th>batch</th>\n",
              "      <th>index in original</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>comments2008c.vrt 2145269</td>\n",
              "      <td>- Tervetuloa skotlantiin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>comments2011c.vrt 3247745</td>\n",
              "      <td>...... No, oikein sopiva sattumaha se vaan oli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>comments2007c.vrt 3792960</td>\n",
              "      <td>40.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>comments2010d.vrt 2351708</td>\n",
              "      <td>Kyseessä voi olla loppuelämäsi nainen.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>comments2007d.vrt 1701675</td>\n",
              "      <td>Sinne vaan ocean clubiin iskemään!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-675ea4f9-2d68-4bff-96ae-32c6a0dd00bf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-675ea4f9-2d68-4bff-96ae-32c6a0dd00bf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-675ea4f9-2d68-4bff-96ae-32c6a0dd00bf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_df = df[df['derive'] > 3]\n",
        "pos_list = list(pos_df['text'])\n",
        "\n",
        "neg_df = df[df['derive'] < 3]\n",
        "neg_list = list(neg_df['text'])"
      ],
      "metadata": {
        "id": "X4xY6VobTiFR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h4Ez_w0VqPF",
        "outputId": "040137f2-d0c2-4ed9-d17e-d3cb55321d42"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['- Tervetuloa skotlantiin...',\n",
              " '...... No, oikein sopiva sattumaha se vaan oli, vai mitä?',\n",
              " 'Kyseessä voi olla loppuelämäsi nainen.',\n",
              " 'Sinne vaan ocean clubiin iskemään!',\n",
              " 'Itsekin pidän Keskustan kampanjointia ihan hyvänä.',\n",
              " 'Muutenkin suosittelen kaikille asiasta kiinnostuneille tuota Mark \"Gravy\" Robertsin mainiota paperia.',\n",
              " 'Ja kun vielä nuukan puoleinenkin on, mikäs sen lepposampaa elää hänen vierellään',\n",
              " 'Tottakai myös ylpeä \"tulevasta\" vaimostani.',\n",
              " 'Suuret kiitokset tämän mahtavan tapahtuman järjestäjille!',\n",
              " 'Kuka \"korkkaisi\" minut?']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eZH_vQ0Vxyx",
        "outputId": "6f841da7-150a-47d3-b1f6-e2797f0affc9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['huomista päivää odotellen..',\n",
              " 'en haluaisi että kissani vuotaa.. =)',\n",
              " 'Nyt olisi lääkitys paikallaan.',\n",
              " 'Eniten pelkään sitä, että jos mies vain koko ajan siirtää perheenperustamista vuosilla eteenpäin, kunnes emme enää saakkaan lapsia..tiedä häntä.',\n",
              " 'Teillähän asenne on kohdallaan!',\n",
              " 'Tuntuu kuin olisin pelkkä huora.',\n",
              " 'en kertonut, koska oisit vaan suuttunut.',\n",
              " 'Missä kohtaa olen sinua nimitellyt?',\n",
              " 'Onko joku tehnyt tälläisen tempun?',\n",
              " 'Sateenkaariväki kuvittelee, että kaksi \"isää\" ei voi korvata äitiä, eikä kaksi \"äitiä\" voi korvata isää.']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to file for local lemmatization\n",
        "\n",
        "def list_to_corp_file(listname, filename):\n",
        "  \"\"\"\n",
        "  Takes a list called listname and output a txt file with each item on one line.\n",
        "  The txt file is named filename (string).\n",
        "  \"\"\"\n",
        "  outfile = open('/content/drive/MyDrive/LGCS124/FinalProject/TranslationTrainingData/' + filename + '.txt', 'w')\n",
        "  for l in listname:\n",
        "    outfile.write(l)\n",
        "    outfile.write('\\n')\n",
        "  outfile.close()"
      ],
      "metadata": {
        "id": "7SX9jILd-jgb"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_to_corp_file(pos_list, 'pos_list')\n",
        "list_to_corp_file(neg_list, 'neg_list')"
      ],
      "metadata": {
        "id": "peRsN5Tc_9_l"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Korpi corpus (Not in use)\n",
        "\n",
        "A corpus analysing sentiment based on the smilely tags. Stopped using since its accuracy of tagging is questionable."
      ],
      "metadata": {
        "id": "2p_z0LNEVIJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_korpi = []\n",
        "neg_korpi = []"
      ],
      "metadata": {
        "id": "HnMQOHbxsuhd"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infile = open('/content/drive/MyDrive/LGCS124/FinalProject/korp_all_sentences.txt', 'r')\n",
        "all = infile.read()\n",
        "\n",
        "infile.close()"
      ],
      "metadata": {
        "id": "MrV8Wl2wVKMr"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned = re.sub(r'\\nb', r'\\n', all)  # remove all the b leading each line\n",
        "cleaned = re.sub('\\n\\'\\:\\)\\'', '', cleaned)  # remove the lines with only :)\n",
        "cleaned = re.sub('\\n\\'\\:\\(\\'', '', cleaned)  # remove the lines with only :(\n",
        "cleaned = re.sub('\\'\\n\\'', '\\n', cleaned)  # remove the quotation marks \n",
        "cleaned = cleaned.replace('\\\\xc3\\\\xa4', 'ä')\n",
        "cleaned = cleaned.replace('\\\\xc3\\\\xb6', 'ö')\n",
        "splitted = cleaned.split('\\n')\n",
        "\n",
        "for line in splitted:\n",
        "  if ':)' in line:\n",
        "    pos_korpi.append(line[:-3])  # Exclude the :)\n",
        "  else:\n",
        "    neg_korpi.append(line[:-3])"
      ],
      "metadata": {
        "id": "UcpdhHcLVMxq"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_korpi[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxSylK4v4FHE",
        "outputId": "5a1490ec-6fd4-4ca1-8128-c41bd8c92a52"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kitystä ei sukupuolella tai iällä ) - Olisin Charlien ja Fredin ja Georgen sekoitus',\n",
              " 'Ja jos jäikin niin mielikuvitus on hoitanut homman',\n",
              " '- Hagrid',\n",
              " 'Ensin ajattelin , että se on varmaan ihan huono , mutta olikin sitten hyvä',\n",
              " 'tai Lily Luna Potter',\n",
              " ') Rowling kirjoittaa hyvin ja tarina on uskomaton',\n",
              " 'ja ron joutuu viettää loppu ikänsä terapeutin vastaan otolla ..',\n",
              " 'jotenki noin se meni',\n",
              " 'njaah Jos Harry kuolee nii se pääse äitinsä , isänsä , Siriuksen ja Dumbledoren luo',\n",
              " 'Jotain olin lainannut kansainvälisten foorumeiden spekulaatioista , mutta ainakin tuo Nevillen vanhempia koskeva osuus oli ihan omani',\n",
              " 'Rowling onnistuu yllättämään aina !',\n",
              " 'Harry ja Voldemortt Nii voihan se tietenkin olla niinkin',\n",
              " 'Enkä usko , että Ginnyäkään hylätään aivan vielä ...',\n",
              " 'Mutta uskon että Nevillellä tulee olemaan suuri rooli viimeisessä kirjassa , kuin myös Lunalla joka loisti poissaolollaan kirjassa 6. Loppuratkaisua en halua edes pohtia , jätän sen Jolle',\n",
              " '10. Dobby on kotitonttu',\n",
              " 'Muuten kiva',\n",
              " 'Ja siirappitorttu kuulostaa niin hyvältä : S Rowling luetteli kirjoissa muitakin herkkuja ja sillon tulee aina nälkä niitä lukiessa',\n",
              " 'Arthur Weasley + Dumbledore + Voldemort Kalkaros Lucius Malfoy Draco Malfoy Vauhkomieli Sirius Musta + Remus Lupin Aika kaikkiruokainen olen',\n",
              " 'Mun on ehottimasti ginny weasley !!!!!!',\n",
              " ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming / Lemmatizing\n",
        "\n",
        "The stemmer is not in use since it does not increase model accuracy.\n",
        "\n",
        "The lemmatizer is from `libvoikko`. Unfortunately, it is not supporting the colab notebook, so I would have to run a script on my local machine."
      ],
      "metadata": {
        "id": "1VsPUIpw6zq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import nltk Snowball stemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "#Create a Finnish instance\n",
        "fistemmer = SnowballStemmer(\"finnish\")\n",
        "\n",
        "#Print the stemmed version of some Finnish word\n",
        "print(fistemmer.stem('toimittaa'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvyag9KSpw7K",
        "outputId": "15ddcd19-3275-4400-e13d-73209d9c187d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toimit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naïve Bayes Training Code"
      ],
      "metadata": {
        "id": "jr1itR2XTq9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import back the list of lemmas\n",
        "\n",
        "pos_lemma_df = pd.read_csv('/content/drive/MyDrive/LGCS124/FinalProject/TranslationTrainingData/pos_lemma.csv')\n",
        "neg_lemma_df = pd.read_csv('/content/drive/MyDrive/LGCS124/FinalProject/TranslationTrainingData/neg_lemma.csv')\n",
        "\n",
        "pos_lemma = list(pos_lemma_df['lemma'])\n",
        "neg_lemma = list(neg_lemma_df['lemma'])"
      ],
      "metadata": {
        "id": "0dYi4Md_Ilwd"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naïve bayes training functions\n",
        "\n",
        "def get_word_counts(word_list):\n",
        "    \"\"\"\n",
        "    Takes as input a lemma list and returns a dictionary with the\n",
        "    number of times each word occurred in that file.\n",
        "    :param list: The file to be counted\n",
        "    :return: A dictionary containing the number of occurrences for each word\n",
        "    \"\"\"\n",
        "\n",
        "    dictionary = {}\n",
        "    #word_list = []\n",
        "\n",
        "    #for l in listname:\n",
        "        #word_list += l.split()\n",
        "\n",
        "    for word in word_list:\n",
        "      word_stemmed = word  # No stemming\n",
        "      #word_stemmed = fistemmer.stem(word)  # stemming\n",
        "      if word_stemmed not in dictionary:\n",
        "          dictionary[word_stemmed] = 1\n",
        "      else:\n",
        "          dictionary[word_stemmed] += 1\n",
        "\n",
        "    return dictionary\n",
        "\n",
        "def counts_to_probs(dictionary, num):\n",
        "    \"\"\"\n",
        "    Takes a dictionary and a number and generates a new dictionary with\n",
        "    the same keys where each value has been divided by the input number.\n",
        "    :param dictionary: The dictionary to be operated on\n",
        "    :param num: The number to be divided\n",
        "    :return: A dictionary in which all values for each key have been divided by num\n",
        "    \"\"\"\n",
        "    for word in dictionary:\n",
        "        dictionary[word] /= num\n",
        "\n",
        "    return dictionary\n",
        "\n",
        "def train_model(listname, word_list):\n",
        "    \"\"\"\n",
        "    Takes as input a listname containing examples and returns a dictionary with the word probabilities.\n",
        "    :param listname: The name of the list to be counted and calculated\n",
        "    :return: A dictionary containing the probability of the occurrences of each word type\n",
        "    \"\"\"\n",
        "    lines = len(listname)\n",
        "    counts = get_word_counts(word_list)\n",
        "    probs = counts_to_probs(counts, lines)\n",
        "    return probs\n",
        "\n",
        "def get_probability(dictionary, string):\n",
        "    \"\"\"\n",
        "    Takes as input two parameters, a dictionary of word probabilities and a\n",
        "    string (representing a review), and returns the probability of that review\n",
        "    by multiplying the probabilities of each of the words in the review.\n",
        "    :param dictionary: The dictionary containing the probability data\n",
        "    :param string: The string to be evaluated on\n",
        "    :return: The product of teh probability of each of the words in the string\n",
        "    \"\"\"\n",
        "    string = string.lower()\n",
        "    word_list = string.split()  # Create a list of words from the input str\n",
        "    probs = 1  # Initialise\n",
        "\n",
        "    for word in word_list:\n",
        "        if word in dictionary:\n",
        "            probs *= dictionary[word]\n",
        "        else:\n",
        "            probs *= 1 / 11000\n",
        "\n",
        "    return probs\n",
        "\n",
        "\n",
        "def classify(string, pos_dict, neg_dict):\n",
        "    \"\"\"\n",
        "    Takes a string (a review), a positive and a negative model. Returns “positive” or\n",
        "    “negative” depending on which model has the highest probability for the review.\n",
        "    Ties goes to positive.\n",
        "    :param string: The review to be classified\n",
        "    :param pos_dict: The positive model (dict)\n",
        "    :param neg_dict: The negative model (dict)\n",
        "    :return: “positive” or “negative” depending on which model has the highest probability for the review.\n",
        "    \"\"\"\n",
        "    pos_probs = get_probability(pos_dict, string)\n",
        "    neg_probs = get_probability(neg_dict, string)\n",
        "    if (pos_probs - neg_probs) >= 0:\n",
        "        return 'positive'\n",
        "    else:\n",
        "        return 'negative'\n",
        "\n",
        "def sentiment_analyzer_interactive(pos_list, neg_list):\n",
        "    \"\"\"\n",
        "    An interactive function that takes two files as input, a positive\n",
        "    examples file and a negative examples file. It would train a positive\n",
        "    and negative model using these files and then repeatedly ask the user\n",
        "    to enter a sentence and then output the classification of that sentence\n",
        "    (as positive or negative). A blank line/sentence should terminate the function.\n",
        "    :param pos_file: File name of the positive training data\n",
        "    :param neg_file: File name of the negative training data\n",
        "    \"\"\"\n",
        "    pos_dict = train_model(pos_list, pos_lemma)\n",
        "    neg_dict = train_model(neg_list, neg_lemma)  # Train the models\n",
        "\n",
        "    print('Blank line terminates.')\n",
        "    string = input('Enter a sentence: ')\n",
        "    while string != '':\n",
        "        print(classify(string, pos_dict, neg_dict))\n",
        "        string = input('Enter a sentence: ')\n",
        "\n",
        "\n",
        "\n",
        "def get_accuracy(pos_train, neg_train, pos_test, neg_test):\n",
        "    \"\"\"\n",
        "    A function that would train the model (i.e., both positive and negative counts) and then classify\n",
        "    all of the test examples (both positive and negative) and keep track of the accuracy of the\n",
        "    model. It would print out three scores: the accuracy on the positive test examples,\n",
        "    the accuracy on the negative test examples, and the accuracy on all of the test examples.\n",
        "    :param pos_test_name: File name of the positive testing data\n",
        "    :param neg_test_name: File name of the negative testing data\n",
        "    :param pos_train: File name of the positive training data\n",
        "    :param neg_train: File name of the negative training data\n",
        "    \"\"\"\n",
        "    pos_dict = train_model(pos_train, pos_lemma)\n",
        "    neg_dict = train_model(neg_train, neg_lemma)  # Train the models\n",
        "\n",
        "    pos_total = len(pos_test)\n",
        "    neg_total = len(neg_test)  # Total num of pos & neg reviews\n",
        "\n",
        "    pos_actual = 0\n",
        "    neg_actual = 0  # Initialise\n",
        "\n",
        "    for l in pos_test:\n",
        "        if classify(l, pos_dict, neg_dict) == 'positive':\n",
        "            pos_actual += 1\n",
        "\n",
        "    for l in neg_test:\n",
        "        if classify(l, pos_dict, neg_dict) == 'negative':\n",
        "            neg_actual += 1\n",
        "\n",
        "    pos_accuracy = pos_actual / pos_total\n",
        "    neg_accuracy = neg_actual / neg_total\n",
        "    total_accuracy = (pos_actual + neg_actual) / (pos_total + neg_total)\n",
        "\n",
        "    print('Positive accuracy: ', pos_accuracy)\n",
        "    print('Negative accuracy: ', neg_accuracy)\n",
        "    print('Total accuracy: ', total_accuracy)"
      ],
      "metadata": {
        "id": "6xfVfkJHTwbi"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(pos_list, neg_list, pos_list, neg_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYowRdUWeMTF",
        "outputId": "28535f11-a3aa-48f2-8d01-37435d633c68"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive accuracy:  0.6641638225255972\n",
            "Negative accuracy:  0.8457292271934922\n",
            "Total accuracy:  0.7749800514229985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer_interactive(pos_list, neg_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdpuaGHsUp_X",
        "outputId": "db046a68-1019-4d8e-af75-d31ad45e0bea"
      },
      "execution_count": 97,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Blank line terminates.\n",
            "Enter a sentence: Jos taas mielestäsi maailmassa on jokin pielessä ja haluat muutosta, osoita tukesi niin liity mukaan muuttamaan maailmaa.\n",
            "positive\n",
            "Enter a sentence: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IHsGLoV7WVtU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}