{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CorpusLingFinalProject.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "2p_z0LNEVIJ5",
        "jr1itR2XTq9-"
      ],
      "authorship_tag": "ABX9TyNV2rOsY1Lp4eB21swmlnmt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xh313/Corpus-Linguistics-Works/blob/main/CorpusLingFinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Planning"
      ],
      "metadata": {
        "id": "8rF6plh7ycKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspiration:\n",
        "![meme](https://user-images.githubusercontent.com/77285010/161146641-d099beed-56d7-40ef-8ae4-0083fe473d6f.png)\n",
        "\n",
        "(source: https://www.reddit.com/r/Suomi/comments/tqx5ac/mielipide_suomesta_eri_alaredditeiss%C3%A4/)\n",
        "\n",
        "When I was learning Finnish through browsing the Finnish language community on reddit r/suomi (meaning Finland in Finnish), I saw this meme which makes a contrast between the English language subreddit on Finland and the native language one. Obviously, the contrast between the sentiment on the place of the natives and the 'tourists' is pretty hilarious. I am thus intrigued to see what the differences are through analysing the corpora!\n"
      ],
      "metadata": {
        "id": "k596vr7xuvRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Idea\n",
        "\n",
        "- Scrap the English and native language subreddits of different places in the world and compare sentiments.\n",
        "- Assume that the English corpora represent the discussions within 'international community', while the native corpora represent the 'local community'.\n",
        "- Compare and contrast:\n",
        "  - English vs Native sentiments\n",
        "  - How the contrast is different for different countries / places around the world.\n",
        "- (?) More detailed analysis on several particular data sets?"
      ],
      "metadata": {
        "id": "a8wNNVUVv5KU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods\n",
        "- Scrap the subreddits\n",
        "- Translation: first compare if the sentiment of the translated corpora is similar enough to the original corpora by sampling a few languages\n",
        "- If translation does not strip the sentiment too much, the translated corpora would be used to do comparative analysis."
      ],
      "metadata": {
        "id": "KQWpV0-yxpNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## [NOT UPDATING] Channel lists:\n",
        "\n",
        "No longer updating since this is too laborious.\n",
        "\n",
        "E = English, N = Native\n",
        "\n",
        "- Poland:\n",
        "  - E: https://www.reddit.com/r/poland/\n",
        "  - N: https://www.reddit.com/r/Polska/\n",
        "\n",
        "- Finland:\n",
        "  - E: https://www.reddit.com/r/Finland/\n",
        "  - N: https://www.reddit.com/r/Suomi/\n",
        "\n",
        "- Brazil:\n",
        "  - E: https://www.reddit.com/r/Brazil/\n",
        "  - N: https://www.reddit.com/r/brasil/\n",
        "\n",
        "- Turkey:\n",
        "  - E: https://www.reddit.com/r/Turkey/\n",
        "  - N: https://www.reddit.com/r/Turkiye/"
      ],
      "metadata": {
        "id": "62IIcmhs2jWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Channel Lists (New):"
      ],
      "metadata": {
        "id": "mWL00b3qMes3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester list to run faster\n",
        "test_srs = ['Finland', 'Suomi', \n",
        "       'Brazil', 'brasil'\n",
        "       ]\n",
        "\n",
        "# Tester list for translation analysis\n",
        "trans_srs = [#'poland', 'Polska',   # The Poland sr does not work properly\n",
        "       'Finland', 'Suomi', \n",
        "       'Brazil', 'brasil',\n",
        "       #'Turkey', 'Turkiye',\n",
        "       'China', 'China_irl',  # Uncertain if this counts\n",
        "       #'japan', 'ja',\n",
        "       #'Thailand', 'thaithai',\n",
        "       #'spain', 'es',  # The 'spain' one is flooded w spanish as well...\n",
        "       #'southafrica', 'RSA',\n",
        "       ]\n",
        "\n",
        "# Current working list\n",
        "# English, Native format!!\n",
        "srs = ['poland', 'Polska',   # The Poland sr does not work properly\n",
        "       'Finland', 'Suomi', \n",
        "       'Brazil', 'brasil',\n",
        "       'Turkey', 'Turkiye',\n",
        "       'China', 'China_irl',  # Uncertain if this counts\n",
        "       'japan', 'ja',\n",
        "       'Thailand', 'thaithai',\n",
        "       'spain', 'es',  # The 'spain' one is flooded w spanish as well...\n",
        "       'southafrica', 'RSA',\n",
        "       'Norway', 'norge',\n",
        "       ]"
      ],
      "metadata": {
        "id": "ImvYf0MSCW4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Always run the list cell after adding new subreddits to make sure that the list is up-to-date"
      ],
      "metadata": {
        "id": "GWZcpX4fMsiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "ZvUms5kAxq5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Current collection method / progress\n",
        "The data I currently need to collect are how effective the different sentiment classifying models are, and what the sentiments of my scraped texts are based on the prediction of the models.\n",
        "\n",
        "To test the model, I wrote a function to get the accuracy by letting my model classify some labelled texts, and then compare the classification as predicted by the model with the actual label originally attached. I controlled the models to test on the same corpus with 2000 positive and 2000 negative sentences, of which randomly 90% is used in training the model and 10% is used in testing. I run each model on the shuffled data 3 times and take the average. The model I decided to use further has an accuracy of 69% for Finnish texts and 75% for English tests when trained on 4000 lines of data, and I would later test if increasing the data size would help increase the accuracy.\n",
        "\n",
        "For more detailed model accuracy [testing results](https://github.com/xh313/Corpus-Linguistics-Works/blob/main/CorpusLingLocalScripts/fi_model_testing.txt) and the [source codes of the model](https://github.com/xh313/Corpus-Linguistics-Works/blob/main/CorpusLingLocalScripts/translated_nltk.py), see the links.\n",
        "\n",
        "To get the sentiment, I simply make my models predict the sentiments of each sentence and post of my text and give me the statistics. I then collect the percentage of posts collected as positive and negative. I also repeat the classification 3 times and take the average. For the translated part, I first translate every sentence, and then run the model trained on English training data to analyze the translated text.\n",
        "\n",
        "For more sentiment raw data see [here](https://github.com/xh313/Corpus-Linguistics-Works/blob/main/CorpusLingLocalScripts/fi_model_result.txt).\n",
        "\n",
        "I have already obtained a sample datum for r/Suomi vs r/Finland. The former one (native forum), when analyzed in Finnish directly, gives out the result that the proportion of posts with positive sentiment is 71.5%; when translated into English and analysed, the proportion is 68.8%, which has a slight discrepancy. The latter one, where discussions are originally held in English, yielded a proportion of 81.8% positive, which is significantly higher than r/Suomi."
      ],
      "metadata": {
        "id": "1mmrtBUnxyIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Issues / Limitations"
      ],
      "metadata": {
        "id": "KPVf9O__yFSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To-be-resolved list\n",
        "\n",
        "1. [Maybe manually build model & train data?] Some native language forums have bits and pieces of English mixed in, or just reddit codes and stuff in English.\n",
        "2. How many hot posts should I extract per forum?\n",
        "3. Naïve Bayes only telling the positive / negative or do topic modelling instead?\n",
        "4. [RESOLVED: lemmatization] Not-so-accurate results for highly-infleceted langs?\n",
        "5. Lacks a good package to remove stopwords from Finnish texts."
      ],
      "metadata": {
        "id": "layPPId-yL-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unresolved limitations\n",
        "\n",
        "1. Size of subreddits vary a lot\n",
        "2. Naturally the topic focus would be subtly different in different subreddits, which ones are comparable?"
      ],
      "metadata": {
        "id": "kyjzv9jWyUY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codes"
      ],
      "metadata": {
        "id": "FtyDqvwsyjB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations / Mounting / Importing"
      ],
      "metadata": {
        "id": "xdUWuwNY41vg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installations"
      ],
      "metadata": {
        "id": "sKicHvp15EPy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiUo8q5Iupna",
        "outputId": "c5c9227c-6968-4637-8c41-6b9bad7cbbee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.5.0-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51 kB 3.3 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 174 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 176 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting prawcore<3,>=2.1\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting websocket-client>=0.54.0\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.18\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n",
            "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
            "Successfully installed praw-7.5.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.3.2\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.10.8)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2021.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 7.6 MB/s \n",
            "\u001b[?25hCollecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 878 kB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16367 sha256=4f9887a46e3570318a5c56c0c84a130d5d3f8ea74e8798fc1db3f62260375553\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/be/fe/93a6a40ffe386e16089e44dad9018ebab9dc4cb9eb7eab65ae\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n",
        "!pip install googletrans==3.1.0a0\n",
        "#!pip install google_trans_new"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing"
      ],
      "metadata": {
        "id": "Z_N54e_g5HBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "\n",
        "reddit = praw.Reddit(client_id=\"R7keGb8iIXeFuLo-CZOs7g\",\n",
        "                     client_secret=\"ieKEfzD3VvRdmg-HGR0yUt3A6jD_Lw\",\n",
        "                     user_agent=\"scrapata\",\n",
        "                     username='hxhthewebscraper',\n",
        "                     password='Puhun_suomea4')\n",
        "\n",
        "print(reddit.user.me())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpfhZPFQ5J7S",
        "outputId": "e9f865f9-4728-47c2-afdd-891d3a9e86b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hxhthewebscraper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from googletrans import Translator\n",
        "#from google_trans_new import google_translator \n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "SKnFVzyUHkC1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGtCPxCO3tLy",
        "outputId": "e33a6920-c7ca-4ef7-e184-85e6b215377c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting"
      ],
      "metadata": {
        "id": "NPT--q3OHbZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iopko5FHHdy8",
        "outputId": "d0a77d80-581d-4e99-e992-449a4f3cb29c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping"
      ],
      "metadata": {
        "id": "aGgGlanM5Rli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subr_to_list(subreddit, number):\n",
        "  \"\"\"\n",
        "  Takes a subreddit name and scraps posts into a list.\n",
        "  Returns the list.\n",
        "  \"\"\"\n",
        "  subr_list = list() #key is body, value is judgement\n",
        "\n",
        "  # get 10 hot posts from the r/Suomi subreddit\n",
        "  hot_posts = reddit.subreddit(subreddit).hot(limit=number) #this is pulling out the hot posts\n",
        "  for post in hot_posts:  \n",
        "      submission = reddit.submission(id=post.id)\n",
        "      subr_list.append(submission.selftext)\n",
        "      #linked_post = reddit.submission(url='https://www.reddit/com/'+post.url)\n",
        "      #for top_level_comment in submission.comments:\n",
        "        #print(top_level_comment.body)\n",
        "        #subr_dict[submission.selftext] = top_level_comment.body\n",
        "\n",
        "  return subr_list"
      ],
      "metadata": {
        "id": "r3n47ufx5QNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_txt(postlist, filename, ordinal='1'):\n",
        "  \"\"\"\n",
        "  Takes a list of processed subreddit posts and output a .txt file.\n",
        "  The ordinal in the end is added to sort the files properly.\n",
        "  \"\"\"\n",
        "  outfile = open('/content/drive/My Drive/LGCS124/FinalProject/TestCorpora/' + str(int(ordinal)) + '_' + filename + '.txt', 'w')\n",
        "\n",
        "  for post in postlist:\n",
        "    outfile.write(post)\n",
        "    outfile.write('\\n')\n",
        "\n",
        "  outfile.close()"
      ],
      "metadata": {
        "id": "TtTzIo3i8_I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester: [DON'T RERUN THIS IT TAKES AGES!!!]\n",
        "\n",
        "#finland_e = subr_to_list('Suomi', 100)"
      ],
      "metadata": {
        "id": "LWwphQBu6kKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list_to_txt(finland_e, 'finland_e')"
      ],
      "metadata": {
        "id": "pWXKAQwx-wI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The ultimate scraper!"
      ],
      "metadata": {
        "id": "dSzYQ9ebBuhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subr_extract(subreddit, number, ordinal='1'):\n",
        "  \"\"\"\n",
        "  The ultimate extraction. Takes a string that is the name of the subreddit\n",
        "  and the number of posts to be scraped.\n",
        "  The ordinal in the end is added to sort the files properly.\n",
        "  \"\"\"\n",
        "  postlist = subr_to_list(subreddit, number)\n",
        "  list_to_txt(postlist, subreddit, ordinal)"
      ],
      "metadata": {
        "id": "syU0rKkC_vbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester\n",
        "subr_extract('Polska', 10)"
      ],
      "metadata": {
        "id": "O1i0cPXMBSaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester\n",
        "ord = 1\n",
        "for sr in test_srs:\n",
        "  subr_extract(sr, 10, ord)\n",
        "  ord += 0.5"
      ],
      "metadata": {
        "id": "-Rl5dWY-C1Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function written to scrape the list [srs](https://colab.research.google.com/drive/1Fh5wP9f8Qt47cjE8xgT86ub9eWUZo-iR#scrollTo=ImvYf0MSCW4B&line=3&uniqifier=1)"
      ],
      "metadata": {
        "id": "eRl5cvzIGPjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrap_srs(currentsrs, number):\n",
        "  ord = 1\n",
        "  for sr in currentsrs:\n",
        "    subr_extract(sr, number, ord)\n",
        "    ord += 0.5"
      ],
      "metadata": {
        "id": "-8i-wlBBGrwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call this to get all files from current srs into the drive\n"
      ],
      "metadata": {
        "id": "8w09pkOeIa5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DON'T RUN UNLESS YOU WANT TO GET THE FILE NOW!\n",
        "#scrap_srs(srs, 30)"
      ],
      "metadata": {
        "id": "2G1vILroIiY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation testing\n",
        "This is to check if the sentiment would be lost after translation of the non-English corpora into English. Two translators would be used:\n",
        "- DeepL\n",
        "- Google Translate\n",
        "\n",
        "To test translation, three pairs of corpora are used:\n",
        "1. Finland (Finnish, English)\n",
        "2. Brazil (Brazilian Portuguese, English)\n",
        "3. China (Simplified Chinese, English)\n",
        "\n",
        "Of which Finnish has the highest morphological complexity being an agglutinative language, Brazilian Portuguese the medium and Chinese the lowest. "
      ],
      "metadata": {
        "id": "kTH6wtSVkimp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code cell to get translation tester corpora\n",
        "scrap_srs(trans_srs, 30)"
      ],
      "metadata": {
        "id": "OX-MD1Imk6qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Finnish sentiment analysis, the testing corpus used is from the Korp API: https://korp.csc.fi/. I referenced https://github.com/tjkemp/ubik-sentiment for the code used to strip data from the corpus. It is combined with another labeled corpus from Turku University."
      ],
      "metadata": {
        "id": "LOikpMifxibk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Corpus input"
      ],
      "metadata": {
        "id": "4S3E3vfPPdVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Turku corpus"
      ],
      "metadata": {
        "id": "HwCvKmjoVF3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/LGCS124/FinalProject/TranslationTrainingData/FinnSentiment2020.tsv', sep='\\t', header=0)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IMoBrIjjPcrt",
        "outputId": "97604d2f-5969-41ed-cb8c-24384990d096"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   a  b  c  major  derive  smiley  produ  split  batch  \\\n",
              "0  1  0  1      1       4       0     -1      1      1   \n",
              "1  0  1  0      0       4       0     -1     12      1   \n",
              "2  0  0  0      0       3       0     -1     14      1   \n",
              "3  1  1  1      1       5       0      1      7      1   \n",
              "4  1  1  1      1       5       0      1     12      1   \n",
              "\n",
              "           index in original  \\\n",
              "0  comments2008c.vrt 2145269   \n",
              "1  comments2011c.vrt 3247745   \n",
              "2  comments2007c.vrt 3792960   \n",
              "3  comments2010d.vrt 2351708   \n",
              "4  comments2007d.vrt 1701675   \n",
              "\n",
              "                                                text  \n",
              "0                        - Tervetuloa skotlantiin...  \n",
              "1  ...... No, oikein sopiva sattumaha se vaan oli...  \n",
              "2                                                40.  \n",
              "3             Kyseessä voi olla loppuelämäsi nainen.  \n",
              "4                 Sinne vaan ocean clubiin iskemään!  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3b185f41-92b4-41d6-b544-1fbcb17b0e2f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>major</th>\n",
              "      <th>derive</th>\n",
              "      <th>smiley</th>\n",
              "      <th>produ</th>\n",
              "      <th>split</th>\n",
              "      <th>batch</th>\n",
              "      <th>index in original</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>comments2008c.vrt 2145269</td>\n",
              "      <td>- Tervetuloa skotlantiin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>comments2011c.vrt 3247745</td>\n",
              "      <td>...... No, oikein sopiva sattumaha se vaan oli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>comments2007c.vrt 3792960</td>\n",
              "      <td>40.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>comments2010d.vrt 2351708</td>\n",
              "      <td>Kyseessä voi olla loppuelämäsi nainen.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>comments2007d.vrt 1701675</td>\n",
              "      <td>Sinne vaan ocean clubiin iskemään!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b185f41-92b4-41d6-b544-1fbcb17b0e2f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3b185f41-92b4-41d6-b544-1fbcb17b0e2f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3b185f41-92b4-41d6-b544-1fbcb17b0e2f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_df = df[df['derive'] > 3]\n",
        "pos_list = list(pos_df['text'])\n",
        "\n",
        "neg_df = df[df['derive'] < 3]\n",
        "neg_list = list(neg_df['text'])\n",
        "\n",
        "neu_df = df[df['derive'] == 3]\n",
        "neu_list = list(neu_df['text'])"
      ],
      "metadata": {
        "id": "X4xY6VobTiFR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h4Ez_w0VqPF",
        "outputId": "00fe9cd8-4110-40c1-ed2c-29b301dbbb43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['- Tervetuloa skotlantiin...',\n",
              " '...... No, oikein sopiva sattumaha se vaan oli, vai mitä?',\n",
              " 'Kyseessä voi olla loppuelämäsi nainen.',\n",
              " 'Sinne vaan ocean clubiin iskemään!',\n",
              " 'Itsekin pidän Keskustan kampanjointia ihan hyvänä.',\n",
              " 'Muutenkin suosittelen kaikille asiasta kiinnostuneille tuota Mark \"Gravy\" Robertsin mainiota paperia.',\n",
              " 'Ja kun vielä nuukan puoleinenkin on, mikäs sen lepposampaa elää hänen vierellään',\n",
              " 'Tottakai myös ylpeä \"tulevasta\" vaimostani.',\n",
              " 'Suuret kiitokset tämän mahtavan tapahtuman järjestäjille!',\n",
              " 'Kuka \"korkkaisi\" minut?']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eZH_vQ0Vxyx",
        "outputId": "999d4138-b3d0-4e7b-f9b9-19eb6101118e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['huomista päivää odotellen..',\n",
              " 'en haluaisi että kissani vuotaa.. =)',\n",
              " 'Nyt olisi lääkitys paikallaan.',\n",
              " 'Eniten pelkään sitä, että jos mies vain koko ajan siirtää perheenperustamista vuosilla eteenpäin, kunnes emme enää saakkaan lapsia..tiedä häntä.',\n",
              " 'Teillähän asenne on kohdallaan!',\n",
              " 'Tuntuu kuin olisin pelkkä huora.',\n",
              " 'en kertonut, koska oisit vaan suuttunut.',\n",
              " 'Missä kohtaa olen sinua nimitellyt?',\n",
              " 'Onko joku tehnyt tälläisen tempun?',\n",
              " 'Sateenkaariväki kuvittelee, että kaksi \"isää\" ei voi korvata äitiä, eikä kaksi \"äitiä\" voi korvata isää.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neu_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av40y47nq_MZ",
        "outputId": "f1f7f180-bbf8-4c3c-e7d8-ab859ccd4a03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['40.',\n",
              " 'Kamppi, Kontula, Kluuvi',\n",
              " 'Ihmiset joutuvat joskus monenlaisien päätelmien kohteeksi.',\n",
              " 'Koska naiset yleensä arvostavat miehessä eniten itsevarmuutta ja ujo mies saattaa helposti antaa itsestään epävarman vaikutelman vaikka ei sitä olisikaan.\\n-1\\t-1\\t-1\\t-1\\t1\\t0\\t1\\t1\\t1\\tcomments2016e.vrt 3255363\\tTyöstä kuuluu maksaa palkkaa vai meinaatko että palkkatyöläiset menisivät 9eurolla hommiin ja maksaisivat sillä laskunsa.Ei hyvää päivää mihin maamme on vajonnut ja hallitus ajaa tälläistä politiikkaa.\\n-1\\t-1\\t0\\t-1\\t2\\t1\\t0\\t3\\t1\\tcomments2010c.vrt 2040506\\tVastahan sinä myönsit, ettei tuolla ominaisuuden hyödyllisyydellä - eikä siten vahingollisuudella - ole mitään tekemistä asian kanssa!',\n",
              " 'Jos taas mielestäsi maailmassa on jokin pielessä ja haluat muutosta, osoita tukesi niin liity mukaan muuttamaan maailmaa.',\n",
              " 'Miten on Turun-lentojen laits?',\n",
              " 'Tämähän ei maksa kaupungille mitään, mutta operaatio säilyttäisi kasarmin.',\n",
              " 'on vapaaehtoista käyttääkö uikkareita vai ei',\n",
              " 'Tuolloin 6-10 suden *lauma* liikuskeli Oriveden lähikunnissa.',\n",
              " 'Tai voit vaikka katella leffoja makuuni.fi sivuilta jos vaikka löytyis joku hyvä leffa minkä vois vuokrata / hankkia.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to file for local lemmatization\n",
        "\n",
        "def list_to_corp_file(listname, filename):\n",
        "  \"\"\"\n",
        "  Takes a list called listname and output a txt file with each item on one line.\n",
        "  The txt file is named filename (string).\n",
        "  \"\"\"\n",
        "  outfile = open('/content/drive/MyDrive/LGCS124/FinalProject/TranslationTrainingData/' + filename + '.txt', 'w')\n",
        "  for l in listname:\n",
        "    outfile.write(l)\n",
        "    outfile.write('\\n')\n",
        "  outfile.close()"
      ],
      "metadata": {
        "id": "7SX9jILd-jgb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_to_corp_file(pos_list, 'pos_list')\n",
        "list_to_corp_file(neg_list, 'neg_list')\n",
        "list_to_corp_file(neu_list, 'neu_list')"
      ],
      "metadata": {
        "id": "peRsN5Tc_9_l"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Korpi corpus (Not in use)\n",
        "\n",
        "A corpus analysing sentiment based on the smilely tags. Stopped using since its accuracy of tagging is questionable (rather than manually tagging the message, it simply search for smiley faces...)."
      ],
      "metadata": {
        "id": "2p_z0LNEVIJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_korpi = []\n",
        "neg_korpi = []"
      ],
      "metadata": {
        "id": "HnMQOHbxsuhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infile = open('/content/drive/MyDrive/LGCS124/FinalProject/korp_all_sentences.txt', 'r')\n",
        "all = infile.read()\n",
        "\n",
        "infile.close()"
      ],
      "metadata": {
        "id": "MrV8Wl2wVKMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned = re.sub(r'\\nb', r'\\n', all)  # remove all the b leading each line\n",
        "cleaned = re.sub('\\n\\'\\:\\)\\'', '', cleaned)  # remove the lines with only :)\n",
        "cleaned = re.sub('\\n\\'\\:\\(\\'', '', cleaned)  # remove the lines with only :(\n",
        "cleaned = re.sub('\\'\\n\\'', '\\n', cleaned)  # remove the quotation marks \n",
        "cleaned = cleaned.replace('\\\\xc3\\\\xa4', 'ä')\n",
        "cleaned = cleaned.replace('\\\\xc3\\\\xb6', 'ö')\n",
        "splitted = cleaned.split('\\n')\n",
        "\n",
        "for line in splitted:\n",
        "  if ':)' in line:\n",
        "    pos_korpi.append(line[:-3])  # Exclude the :)\n",
        "  else:\n",
        "    neg_korpi.append(line[:-3])"
      ],
      "metadata": {
        "id": "UcpdhHcLVMxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_korpi[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxSylK4v4FHE",
        "outputId": "5a1490ec-6fd4-4ca1-8128-c41bd8c92a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kitystä ei sukupuolella tai iällä ) - Olisin Charlien ja Fredin ja Georgen sekoitus',\n",
              " 'Ja jos jäikin niin mielikuvitus on hoitanut homman',\n",
              " '- Hagrid',\n",
              " 'Ensin ajattelin , että se on varmaan ihan huono , mutta olikin sitten hyvä',\n",
              " 'tai Lily Luna Potter',\n",
              " ') Rowling kirjoittaa hyvin ja tarina on uskomaton',\n",
              " 'ja ron joutuu viettää loppu ikänsä terapeutin vastaan otolla ..',\n",
              " 'jotenki noin se meni',\n",
              " 'njaah Jos Harry kuolee nii se pääse äitinsä , isänsä , Siriuksen ja Dumbledoren luo',\n",
              " 'Jotain olin lainannut kansainvälisten foorumeiden spekulaatioista , mutta ainakin tuo Nevillen vanhempia koskeva osuus oli ihan omani',\n",
              " 'Rowling onnistuu yllättämään aina !',\n",
              " 'Harry ja Voldemortt Nii voihan se tietenkin olla niinkin',\n",
              " 'Enkä usko , että Ginnyäkään hylätään aivan vielä ...',\n",
              " 'Mutta uskon että Nevillellä tulee olemaan suuri rooli viimeisessä kirjassa , kuin myös Lunalla joka loisti poissaolollaan kirjassa 6. Loppuratkaisua en halua edes pohtia , jätän sen Jolle',\n",
              " '10. Dobby on kotitonttu',\n",
              " 'Muuten kiva',\n",
              " 'Ja siirappitorttu kuulostaa niin hyvältä : S Rowling luetteli kirjoissa muitakin herkkuja ja sillon tulee aina nälkä niitä lukiessa',\n",
              " 'Arthur Weasley + Dumbledore + Voldemort Kalkaros Lucius Malfoy Draco Malfoy Vauhkomieli Sirius Musta + Remus Lupin Aika kaikkiruokainen olen',\n",
              " 'Mun on ehottimasti ginny weasley !!!!!!',\n",
              " ':)']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming (Not in use)\n",
        "\n",
        "The stemmer is not in use since it does not increase model accuracy and fails to stem same words with different inflections correctly."
      ],
      "metadata": {
        "id": "1VsPUIpw6zq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import nltk Snowball stemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "#Create a Finnish instance\n",
        "fistemmer = SnowballStemmer(\"finnish\")\n",
        "\n",
        "#Print the stemmed version of some Finnish word\n",
        "print(fistemmer.stem('toimittaa'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvyag9KSpw7K",
        "outputId": "15ddcd19-3275-4400-e13d-73209d9c187d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toimit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Self-built Naïve Bayes Training w/ Lemmatization (Not in use, moved to local machine instead)\n",
        "\n",
        "Since the library and dictionary used for lemmatization is too huge, this part is run locally. See my code [here](https://github.com/xh313/Corpus-Linguistics-Works/tree/main/CorpusLingLocalScripts). The lemmatizing database is from `libvoikko`. For info about this library see [here](https://voikko.puimula.org/).\n"
      ],
      "metadata": {
        "id": "jr1itR2XTq9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import back the list of lemma\n",
        "\n",
        "pos_lemma_df = pd.read_csv('/content/drive/MyDrive/LGCS124/FinalProject/TranslationTrainingData/pos_lemma.csv')\n",
        "neg_lemma_df = pd.read_csv('/content/drive/MyDrive/LGCS124/FinalProject/TranslationTrainingData/neg_lemma.csv')\n",
        "\n",
        "pos_lemma = list(pos_lemma_df['lemma'])\n",
        "neg_lemma = list(neg_lemma_df['lemma'])"
      ],
      "metadata": {
        "id": "0dYi4Md_Ilwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naïve bayes training functions\n",
        "\n",
        "def get_word_counts(word_list):\n",
        "    \"\"\"\n",
        "    Takes as input a lemma list and returns a dictionary with the\n",
        "    number of times each word occurred in that file.\n",
        "    :param list: The file to be counted\n",
        "    :return: A dictionary containing the number of occurrences for each word\n",
        "    \"\"\"\n",
        "\n",
        "    dictionary = {}\n",
        "    #word_list = []\n",
        "\n",
        "    #for l in listname:\n",
        "        #word_list += l.split()\n",
        "\n",
        "    for word in word_list:\n",
        "      word_stemmed = word  # No stemming\n",
        "      #word_stemmed = fistemmer.stem(word)  # stemming\n",
        "      if word_stemmed not in dictionary:\n",
        "          dictionary[word_stemmed] = 1\n",
        "      else:\n",
        "          dictionary[word_stemmed] += 1\n",
        "\n",
        "    return dictionary\n",
        "\n",
        "def counts_to_probs(dictionary, num):\n",
        "    \"\"\"\n",
        "    Takes a dictionary and a number and generates a new dictionary with\n",
        "    the same keys where each value has been divided by the input number.\n",
        "    :param dictionary: The dictionary to be operated on\n",
        "    :param num: The number to be divided\n",
        "    :return: A dictionary in which all values for each key have been divided by num\n",
        "    \"\"\"\n",
        "    for word in dictionary:\n",
        "        dictionary[word] /= num\n",
        "\n",
        "    return dictionary\n",
        "\n",
        "def train_model(listname, word_list):\n",
        "    \"\"\"\n",
        "    Takes as input a listname containing examples and returns a dictionary with the word probabilities.\n",
        "    :param listname: The name of the list to be counted and calculated\n",
        "    :return: A dictionary containing the probability of the occurrences of each word type\n",
        "    \"\"\"\n",
        "    lines = len(listname)\n",
        "    counts = get_word_counts(word_list)\n",
        "    probs = counts_to_probs(counts, lines)\n",
        "    return probs\n",
        "\n",
        "def get_probability(dictionary, string):\n",
        "    \"\"\"\n",
        "    Takes as input two parameters, a dictionary of word probabilities and a\n",
        "    string (representing a review), and returns the probability of that review\n",
        "    by multiplying the probabilities of each of the words in the review.\n",
        "    :param dictionary: The dictionary containing the probability data\n",
        "    :param string: The string to be evaluated on\n",
        "    :return: The product of teh probability of each of the words in the string\n",
        "    \"\"\"\n",
        "    string = string.lower()\n",
        "    word_list = string.split()  # Create a list of words from the input str\n",
        "    probs = 1  # Initialise\n",
        "\n",
        "    for word in word_list:\n",
        "        if word in dictionary:\n",
        "            probs *= dictionary[word]\n",
        "        else:\n",
        "            probs *= 1 / 11000\n",
        "\n",
        "    return probs\n",
        "\n",
        "\n",
        "def classify(string, pos_dict, neg_dict):\n",
        "    \"\"\"\n",
        "    Takes a string (a review), a positive and a negative model. Returns “positive” or\n",
        "    “negative” depending on which model has the highest probability for the review.\n",
        "    Ties goes to positive.\n",
        "    :param string: The review to be classified\n",
        "    :param pos_dict: The positive model (dict)\n",
        "    :param neg_dict: The negative model (dict)\n",
        "    :return: “positive” or “negative” depending on which model has the highest probability for the review.\n",
        "    \"\"\"\n",
        "    pos_probs = get_probability(pos_dict, string)\n",
        "    neg_probs = get_probability(neg_dict, string)\n",
        "    if (pos_probs - neg_probs) >= 0:\n",
        "        return 'positive'\n",
        "    else:\n",
        "        return 'negative'\n",
        "\n",
        "def sentiment_analyzer_interactive(pos_list, neg_list):\n",
        "    \"\"\"\n",
        "    An interactive function that takes two files as input, a positive\n",
        "    examples file and a negative examples file. It would train a positive\n",
        "    and negative model using these files and then repeatedly ask the user\n",
        "    to enter a sentence and then output the classification of that sentence\n",
        "    (as positive or negative). A blank line/sentence should terminate the function.\n",
        "    :param pos_file: File name of the positive training data\n",
        "    :param neg_file: File name of the negative training data\n",
        "    \"\"\"\n",
        "    pos_dict = train_model(pos_list, pos_lemma)\n",
        "    neg_dict = train_model(neg_list, neg_lemma)  # Train the models\n",
        "\n",
        "    print('Blank line terminates.')\n",
        "    string = input('Enter a sentence: ')\n",
        "    while string != '':\n",
        "        print(classify(string, pos_dict, neg_dict))\n",
        "        string = input('Enter a sentence: ')\n",
        "\n",
        "\n",
        "\n",
        "def get_accuracy(pos_train, neg_train, pos_test, neg_test):\n",
        "    \"\"\"\n",
        "    A function that would train the model (i.e., both positive and negative counts) and then classify\n",
        "    all of the test examples (both positive and negative) and keep track of the accuracy of the\n",
        "    model. It would print out three scores: the accuracy on the positive test examples,\n",
        "    the accuracy on the negative test examples, and the accuracy on all of the test examples.\n",
        "    :param pos_test_name: File name of the positive testing data\n",
        "    :param neg_test_name: File name of the negative testing data\n",
        "    :param pos_train: File name of the positive training data\n",
        "    :param neg_train: File name of the negative training data\n",
        "    \"\"\"\n",
        "    pos_dict = train_model(pos_train, pos_lemma)\n",
        "    neg_dict = train_model(neg_train, neg_lemma)  # Train the models\n",
        "\n",
        "    pos_total = len(pos_test)\n",
        "    neg_total = len(neg_test)  # Total num of pos & neg reviews\n",
        "\n",
        "    pos_actual = 0\n",
        "    neg_actual = 0  # Initialise\n",
        "\n",
        "    for l in pos_test:\n",
        "        if classify(l, pos_dict, neg_dict) == 'positive':\n",
        "            pos_actual += 1\n",
        "\n",
        "    for l in neg_test:\n",
        "        if classify(l, pos_dict, neg_dict) == 'negative':\n",
        "            neg_actual += 1\n",
        "\n",
        "    pos_accuracy = pos_actual / pos_total\n",
        "    neg_accuracy = neg_actual / neg_total\n",
        "    total_accuracy = (pos_actual + neg_actual) / (pos_total + neg_total)\n",
        "\n",
        "    print('Positive accuracy: ', pos_accuracy)\n",
        "    print('Negative accuracy: ', neg_accuracy)\n",
        "    print('Total accuracy: ', total_accuracy)"
      ],
      "metadata": {
        "id": "6xfVfkJHTwbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(pos_list, neg_list, pos_korpi, neg_korpi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYowRdUWeMTF",
        "outputId": "9317664c-d14a-4b79-fc8e-96149299a9a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive accuracy:  0.3932444645277903\n",
            "Negative accuracy:  0.7922077922077922\n",
            "Total accuracy:  0.43095335515548283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer_interactive(pos_list, neg_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdpuaGHsUp_X",
        "outputId": "db046a68-1019-4d8e-af75-d31ad45e0bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Blank line terminates.\n",
            "Enter a sentence: Jos taas mielestäsi maailmassa on jokin pielessä ja haluat muutosta, osoita tukesi niin liity mukaan muuttamaan maailmaa.\n",
            "positive\n",
            "Enter a sentence: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model choice and comparisons\n",
        "\n",
        "In addition to my self-built model, several other built classifier models are used, including the NLTK Naïve Bayes, Scikit-learn Naïve Bayes (Bernoulli), and Scikit-learn Support Vector Machine.\n",
        "\n",
        "For the comprehensive comparison result, see [here](https://github.com/xh313/Corpus-Linguistics-Works/blob/main/CorpusLingLocalScripts/fi_model_testing.txt).\n",
        "\n"
      ],
      "metadata": {
        "id": "eAs6eKaBNASo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Translating & English sentiment\n",
        "\n",
        "Google translation and google translator API is used for translation. \n",
        "\n",
        "TextBlob is used for English sentiment analysis in order to compare with the source language results.\n",
        "\n",
        "The TextBlob.sentiment method also uses the NLTK Naïve Bayes classifier, which is the same algorithm as the one I built for Finnish."
      ],
      "metadata": {
        "id": "VBep2kCjf_m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Emoji stripper\n",
        "def strip_emoji(text):\n",
        "  all_emoji = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
        "  return all_emoji.sub(r'', text)"
      ],
      "metadata": {
        "id": "393n5bdNGeBa"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob.blob import Sentence\n",
        "def analyze_in_eng(subreddit, ordinal=1):\n",
        "  \"\"\"\n",
        "  Translate the file named the same as the subreddit, then analyse the\n",
        "  sentiment of the text.\n",
        "  Returns the positive/negative data dict.\n",
        "  Look for the filename and its ordinal in the drive!\n",
        "  \"\"\"\n",
        "  translator = Translator()  # To use the google api\n",
        "\n",
        "  infile = open('/content/drive/My Drive/LGCS124/FinalProject/TestCorpora/' \n",
        "                + str(int(ordinal)) + '_' + subreddit + '.txt')\n",
        "  pos_count = 0\n",
        "  neg_count = 0\n",
        "  pos_posts = []\n",
        "  neg_posts = []\n",
        "  neu_posts = []\n",
        "  polarity_all_posts = []\n",
        "\n",
        "  for line in infile:\n",
        "    line_clean = strip_emoji(line)  # Remove emoji\n",
        "    line_clean = re.sub(r'http\\S+', '', line_clean)  # Remove links\n",
        "    line_clean = re.sub(r'\\s{2,}', ' ', line_clean)  # Remove extra spaces\n",
        "    line_clean = re.sub(r'\\*', '', line_clean)  # Remove asterisks\n",
        "    \n",
        "    lineblob = TextBlob(line_clean)  # Create a textblob\n",
        "    polarity_all_sentences = []  # Initialise\n",
        "    for sentence in lineblob.sentences:\n",
        "      lang = translator.detect(str(sentence)).lang\n",
        "      if lang != 'en':\n",
        "        sentence = sentence.translate(to='en')  # Into English!\n",
        "      print(f'Sentence: {sentence}')\n",
        "      polarity = sentence.sentiment.polarity\n",
        "      print(f'Polarity: {polarity}')\n",
        "      polarity_all_sentences.append(polarity)\n",
        "      polarity_all_posts.append(polarity)\n",
        "    \n",
        "    average_polarity = np.average(polarity_all_sentences)\n",
        "    if average_polarity < 0:\n",
        "      neg_count += 1\n",
        "      neg_posts.append(line)\n",
        "    if average_polarity > 0:\n",
        "      pos_count += 1\n",
        "      pos_posts.append(line)\n",
        "    else:\n",
        "      neu_posts.append(line)\n",
        "\n",
        "  # Calculate the proportion of positive posts\n",
        "  proportion = pos_count / (pos_count + neg_count)\n",
        "  avg_polarity = np.average(polarity_all_posts)\n",
        "\n",
        "  # Sort a dict of data\n",
        "  data = {'pos': pos_count, 'neg': neg_count,\n",
        "          'prop': proportion, 'pol': avg_polarity,\n",
        "          'pos_posts': pos_posts, 'neg_posts': neg_posts,\n",
        "          'neu_posts': neu_posts}\n",
        "  \n",
        "  # Print the data of the file analysed\n",
        "  print(f'{subreddit}, translated version')\n",
        "  print('Positives: ' + str(pos_count))\n",
        "  print('Negatives: ' + str(neg_count))\n",
        "  print('Portion Positive: ' + str(proportion))\n",
        "  print(f'Average Polarity: {avg_polarity}')\n",
        "\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "g5L3OBiwjFSt"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester TAKES EXTRA LONG CAUTION\n",
        "analyze_in_eng('Suomi', 1)\n",
        "\n",
        "print('End marker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR6X4Ugq8M6L",
        "outputId": "5e20fb0d-d92e-439d-edb5-6b0e8bb11028"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: All links, updates and reflections from Ukraine's evolving conflict, and other reactions and countermeasures of the rest of the world, as a rule for this chain.\n",
            "Polarity: -0.125\n",
            "Sentence: New posts related to the topic are deleted and directed here.\n",
            "Polarity: 0.06818181818181818\n",
            "Sentence: ### Live tracking\n",
            "Polarity: 0.13636363636363635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: - [YLE] (\n",
            "Polarity: 0.0\n",
            "Sentence: - [Helsingin sanomat newspaper](\n",
            "Polarity: 0.0\n",
            "Sentence: ### Other links\n",
            "Polarity: -0.125\n",
            "Sentence: [Defense School - Situation of the Ukrainian War] (\n",
            "Polarity: 0.0\n",
            "Sentence: \"We will publish this site daily by 14 at 14:00 of the public sources from the Status Overview of the Ukrainian War.\"\n",
            "Polarity: 0.0\n",
            "Sentence: [Defense Forces: Situation in Ukraine] (\n",
            "Polarity: 0.0\n",
            "Sentence: - [LiveMap from Ukraine] (- a updated map of the conflict (page may need to be updated before the link works).\n",
            "Polarity: 0.0\n",
            "Sentence: WARNING: Image updates may contain images of fatalities, etc.\n",
            "Polarity: 0.0\n",
            "Sentence: - Another [Situation Map of Ukraine] (in Finnish.\n",
            "Polarity: 0.0\n",
            "Sentence: - tips and discussions on how R / Finn could financially support the Ukrainian people [in this chain] (\n",
            "Polarity: 0.0\n",
            "Sentence: - [If you want to help Ukrainians don't work at least this: you can cause more trouble - with these tips assistance goes to the helper] (\n",
            "Polarity: 0.0\n",
            "Sentence: - [the mind] (the crisis phone calls and provides a conversation assistance 24/7: 09 2525 0111.\n",
            "Polarity: 0.0\n",
            "Sentence: ### A few meta-attention:\n",
            "Polarity: -0.2\n",
            "Sentence: - The yarn has [a serious conversation] (black flair, and modernize it accordingly.\n",
            "Polarity: -0.19999999999999998\n",
            "Sentence: - Only because someone in a controversial topic disagrees with you, does not make him \"Troll\" or \"Botti\".\n",
            "Polarity: 0.275\n",
            "Sentence: This does not help, and not a genuine bottle stops, the fact that you leave the right conversation with accusations and solids.\n",
            "Polarity: 0.042857142857142844\n",
            "Sentence: The right nationhoms are less likely to try to gain popularity with rational argumentation, but specifically gets visibility with Provo - Provo, under which there is a lot of barking, there is a successful troll.\n",
            "Polarity: 0.21726190476190477\n",
            "Sentence: YLE [article] (from the topic of 2015.\n",
            "Polarity: 0.0\n",
            "Sentence: If you genuinely doubt that someone's ID in r / in Finland is a foreign vulnerability of a foreign power, reporting or sending [modmail] (\n",
            "Polarity: 0.05000000000000001\n",
            "Sentence: - This should not be reminded, but on the basis of the reporting queue of recent days, the \"Risk\" has [degrading word] (and its use in most of the machinery is interpreted as racist discrimination and refrigerated accordingly.\n",
            "Polarity: 0.25\n",
            "Sentence: The Russian state's immoral actions are not the reason to bounce anger against the Finnish population.\n",
            "Polarity: -0.35\n",
            "Sentence: ### Why megalanka?\n",
            "Polarity: 0.0\n",
            "Sentence: Upgrades and discussion on the on-site conflict are easiest to follow (and modernize) if they can be found in one place, and not fragmented around Alereddit.\n",
            "Polarity: 0.0\n",
            "Sentence: As a rule, all relevant new posts are directed here.\n",
            "Polarity: 0.2681818181818182\n",
            "Sentence: If one of the topic of the subject is a clear distinctive overallness, or if new major significant twists, some posts may be omitted to their own yarn.\n",
            "Polarity: 0.14388528138528137\n",
            "Sentence: But the debate is generally not promoting eg\n",
            "Polarity: -0.02500000000000001\n",
            "Sentence: The fact that each explosion is in r / in Finland with its own yarn.\n",
            "Polarity: 0.25\n",
            "Sentence: Also, Meemu about the subject is limited - except that this melemulu on the topic while reporting on the first deadlines of war will be dangerously beneficial to the borders, so the situation of the home page began to be that the Heemuilu was in danger of drowning a serious debate around the subject.\n",
            "Polarity: -0.18134920634920634\n",
            "Sentence: [Link to the previous megacker.\n",
            "Polarity: -0.16666666666666666\n",
            "Sentence: ](\n",
            "Polarity: 0.0\n",
            "Sentence: I personally hope that your state agencies and also the private sector are taking this seriously.\n",
            "Polarity: -0.1111111111111111\n",
            "Sentence: We're keeping our fingers crossed for you and hope that all turns out well and whatever you decide, it's based on reason and the real will of the finnish people!\n",
            "Polarity: 0.25\n",
            "Sentence: If there is something positive or negative experiences to make a comment\n",
            "Polarity: -0.03636363636363636\n",
            "Sentence: The most truth of such news, where is the top just reasonable, but gradually goes more and more language to the cheek until at the end of the disappointment how this was a full joke whole article.\n",
            "Polarity: 0.23888888888888893\n",
            "Sentence: Then open the next news and realizes that this aprilliopila was in the previous news the truth was just a more wonderful.\n",
            "Polarity: 0.2666666666666667\n",
            "Sentence: Now I am going to mind to lighten with the peoples on the basis of their own experiences.\n",
            "Polarity: 0.6\n",
            "Sentence: This topic concerns house owners or other residences that meet their renovations themselves.\n",
            "Polarity: -0.125\n",
            "Sentence: Or such intentionally.\n",
            "Polarity: 0.0\n",
            "Sentence: A few times a year on the door will appear in a nice guy with an important thing.\n",
            "Polarity: 0.26666666666666666\n",
            "Sentence: They are just in the area of ​​making / designing a certain type of renovation (roof, window, solar panel, parquet, rack, painting) and thought about the question if the need would be.\n",
            "Polarity: 0.21428571428571427\n",
            "Sentence: Then a free estimate (sales visit) will then be agreed.\n",
            "Polarity: 0.4\n",
            "Sentence: The visits have the same formula:\n",
            "Polarity: 0.0\n",
            "Sentence: - The assessor (seller) is comfortable and occurs as an expert convincingly.\n",
            "Polarity: 0.45\n",
            "Sentence: The beliefs are asking and talking about a nice family of hobbies, etc.\n",
            "Polarity: 0.6\n",
            "Sentence: - In fact, the estimate of the house is praised, but the renovation offered to them would be a need for renovation.\n",
            "Polarity: 0.0\n",
            "Sentence: A couple of obvious facts about building regulations are discussed between the jude, with a couple of conscience.\n",
            "Polarity: 0.0\n",
            "Sentence: - The little firma is presented.\n",
            "Polarity: -0.1875\n",
            "Sentence: The firma is usually domestic, credit rating and operates in a wide area.\n",
            "Polarity: -0.05\n",
            "Sentence: The colleagues are nice and are clearly proud of their firms.\n",
            "Polarity: 0.7\n",
            "Sentence: Hardness is a lot of growth and customers, and with someone's barometer, customers are happy.\n",
            "Polarity: 0.8\n",
            "Sentence: Installers are promised to be Finnish professional skills.\n",
            "Polarity: 0.1\n",
            "Sentence: - It is told a bit of renovation and what it gets everything.\n",
            "Polarity: 0.0\n",
            "Sentence: The value of the property will rise and in fact savings becomes because of its bigger renovation.\n",
            "Polarity: 0.0\n",
            "Sentence: The renovation is described as an essential and good investment.\n",
            "Polarity: 0.35\n",
            "Sentence: At this stage, examples of exemplary locations are also displayed.\n",
            "Polarity: 0.0\n",
            "Sentence: - The work also promises documents and certificates and possibly also warranty.\n",
            "Polarity: 0.0\n",
            "Sentence: - The postponement of renovation is not directly painted by threats, but appropriately designed that serious consequences may come if it does not work.\n",
            "Polarity: 0.038888888888888896\n",
            "Sentence: - Everything would be easy, risk-free and fast.\n",
            "Polarity: 0.3444444444444445\n",
            "Sentence: Avamet with the principle of your hand.\n",
            "Polarity: 0.0\n",
            "Sentence: - There is nothing to talk about the price.\n",
            "Polarity: 0.0\n",
            "Sentence: - Sit to tear someone ready for the silt.\n",
            "Polarity: 0.2\n",
            "Sentence: There is a little bit of pictures and counting stuff.\n",
            "Polarity: -0.1875\n",
            "Sentence: Additional services will be chased about the sophisticated questions to the lapse.\n",
            "Polarity: 0.5\n",
            "Sentence: - The calculation culminates that a tight price tailored to Just suel has been brought up.\n",
            "Polarity: -0.17857142857142858\n",
            "Sentence: At this point, it is emphasized that the price is low because the work is centrally made by region and the same quality is high.\n",
            "Polarity: 0.04\n",
            "Sentence: The financial statement will, at the same time, does not matter if there is no money immediately and anyway, the decline will only come after you have acknowledged the job.\n",
            "Polarity: 0.0\n",
            "Sentence: - The household reductions are calculated to be completed and ultimately you will not even pay even much.\n",
            "Polarity: 0.1\n",
            "Sentence: - If you go to the barrier, then the evaluator calls to one of his bosses and negotiates a discount on it.\n",
            "Polarity: 0.0\n",
            "Sentence: It usually takes time nominally (~ 5%), but however, that the price will become less in your eyes.\n",
            "Polarity: -0.20833333333333331\n",
            "Sentence: - The name is to be desired immediately.\n",
            "Polarity: 0.0\n",
            "Sentence: They have a hurry and are just moving to the next community to make estimates.\n",
            "Polarity: 0.0\n",
            "Sentence: - Sales speech and emphasis on the uniqueness of the event will continue without a break if the name does not arise in the paper.\n",
            "Polarity: 0.0\n",
            "Sentence: - Sometimes these types are awkward to get rid of.\n",
            "Polarity: -0.6\n",
            "Sentence: According to your experience, they should be sent out and leave the service without buying.\n",
            "Polarity: 0.0\n",
            "Sentence: I do not argue that these firms would be cheaters.\n",
            "Polarity: 0.0\n",
            "Sentence: Certainly, work should be done and the trace would be good.\n",
            "Polarity: 0.45714285714285713\n",
            "Sentence: The renovation of the renovation is often real because it is relatively easy to target visits to some houses.\n",
            "Polarity: 0.31666666666666665\n",
            "Sentence: That price only is 1.5 to 3 fold compared to the corresponding equivalent domestic professional work.\n",
            "Polarity: 0.03333333333333333\n",
            "Sentence: Their business is based on strong-targeted sales and rapid trade without competition.\n",
            "Polarity: 0.0\n",
            "Sentence: They seek to create a familiar, honest and reliable picture of themselves and self-selling service is a necessary and unique opportunity.\n",
            "Polarity: 0.3375\n",
            "Sentence: Examples:\n",
            "Polarity: 0.0\n",
            "Sentence: Sewer brush wash:\n",
            "Polarity: 0.0\n",
            "Sentence: Tinged by Avian 2000E.\n",
            "Polarity: 0.0\n",
            "Sentence: Competitive 700e\n",
            "Polarity: 0.0\n",
            "Sentence: Solar panels installed:\n",
            "Polarity: 0.0\n",
            "Sentence: Tinged door to 15,000 e\n",
            "Polarity: 0.0\n",
            "Sentence: A large domestic operator's corresponding turnkey package without a competition: 8000 e\n",
            "Polarity: 0.10714285714285714\n",
            "Sentence: Painting of outer walls:\n",
            "Polarity: 0.0\n",
            "Sentence: Tinged offer price at the door: 9000 e\n",
            "Polarity: 0.0\n",
            "Sentence: Local Painting Firma, Same Paints: 5500 E\n",
            "Polarity: 0.0\n",
            "Sentence: Shift of windows:\n",
            "Polarity: 0.0\n",
            "Sentence: A doctrine tinging: EUR 26 000\n",
            "Polarity: 0.0\n",
            "Sentence: My estimate.\n",
            "Polarity: 0.0\n",
            "Sentence: No exchange of exchange.\n",
            "Polarity: 0.0\n",
            "Sentence: I renovate yourself.\n",
            "Polarity: 0.0\n",
            "Sentence: I would always recommend to compete.\n",
            "Polarity: 0.0\n",
            "Sentence: I'm not in 8 years really bumped to a good door price.\n",
            "Polarity: 0.44999999999999996\n",
            "Sentence: Never is not so intense that you can not get acquainted with themselves.\n",
            "Polarity: 0.35\n",
            "Sentence: My own bunches are then these \"Irish\" renovations that are not worth to let alone.\n",
            "Polarity: 0.15\n",
            "Sentence: [\n",
            "Polarity: 0.0\n",
            "Sentence: I work in the financial sector with issues related to regulations.\n",
            "Polarity: 0.0\n",
            "Sentence: My boss may be experienced and smartly compared to my previous pre-spots.\n",
            "Polarity: 0.28253968253968254\n",
            "Sentence: Still, he is a fairly direct and cold-shaped communication, so between it is difficult to explain whether he is happy after my work or not.\n",
            "Polarity: 0.13333333333333333\n",
            "Sentence: Fairly demanding also, not so much to say that loudly, but somehow it exudes the essence.\n",
            "Polarity: 0.3333333333333333\n",
            "Sentence: Today, I asked an annual salary for the annual salary and said that it was already taken into account when I was hired (some months ago).\n",
            "Polarity: 0.0\n",
            "Sentence: I therefore exchanged units within the firm and I livened their goals in the previous delusions.\n",
            "Polarity: -0.18333333333333335\n",
            "Sentence: This was a fairly straight vale boss because I have had no information on my work input before.\n",
            "Polarity: 0.2\n",
            "Sentence: Would not hurt otherwise because my salary is good, but when you had to go such a mother to say so would make the mind to challenge bringing his crap speech.\n",
            "Polarity: -0.03333333333333336\n",
            "Sentence: On the other hand, a bit will heat up to Kithee already in a good computation, but on the other hand, its increase should be based on the Last Year's work.\n",
            "Polarity: 0.11249999999999999\n",
            "Sentence: Satan, what would you answer that now.\n",
            "Polarity: 0.0\n",
            "Sentence: If you want so tell your own bosses (happy bad) so maybe my mind will get better or no.\n",
            "Polarity: 0.30000000000000004\n",
            "Sentence: Now, there is a very clear hybridity in progress [YLE's commitment section of Saul's native membership of Saul] (all may not be Russian trolls, but a big part of the time sure.\n",
            "Polarity: 0.1575\n",
            "Sentence: There are many NATO-anti-competitive comments in the debate and in proportion to less NATO-positive or fact remedial comments on multiple liking.\n",
            "Polarity: 0.11111111111111112\n",
            "Sentence: The lies and strange opinions seen in the comments:\n",
            "Polarity: -0.05\n",
            "Sentence: All kinds of MISISFORMAIA NATO?\n",
            "Polarity: 0.0\n",
            "Sentence: A reduction in the amount of polls made\n",
            "Polarity: 0.0\n",
            "Sentence: Sampling\n",
            "Polarity: 0.0\n",
            "Sentence: Lies on sampling methods\n",
            "Polarity: 0.0\n",
            "Sentence: Lies on the questions\n",
            "Polarity: 0.0\n",
            "Sentence: Finland's EU membership has violated relations with Russia.\n",
            "Polarity: 0.0\n",
            "Sentence: The World War lights up if NATO joins.\n",
            "Polarity: 0.0\n",
            "Sentence: NATO is expensive for Finland.\n",
            "Polarity: -0.5\n",
            "Sentence: Only a direct referendum is democracy.\n",
            "Polarity: 0.05\n",
            "Sentence: Hybridism does not affect Finns, it is not worth afraid.\n",
            "Polarity: -0.375\n",
            "Sentence: Must expect five years that the people's true opinion will be raised.\n",
            "Polarity: 0.35\n",
            "Sentence: People are only temporarily in shock on this \"crisis\".\n",
            "Polarity: 0.0\n",
            "Sentence: ~ Business of Russia has a long history in Finland. ~~\n",
            "Polarity: -0.05\n",
            "Sentence: Nuclear weapons have been placed in Norway despite the terms of the NATO Agreement.\n",
            "Polarity: 0.0\n",
            "Sentence: ~~ NATO membership should be voted since EU membership was voted on ~~.\n",
            "Polarity: 0.0\n",
            "Sentence: For us Finns have always been lied to the media.\n",
            "Polarity: 0.0\n",
            "Sentence: That is, be accurate with the facts in NATO discussions in all, trollitils are subject to overtime in Finland.\n",
            "Polarity: 0.11666666666666671\n",
            "Sentence: Edit: The underlined points do not in themselves have enough obscure throws to belong to the group.\n",
            "Polarity: 0.0\n",
            "Sentence: Russia's antijack was raised but a little bit about an income angle of the mile, and mixed the EU vote for the euro, which was not considered.\n",
            "Polarity: -0.09375\n",
            "Sentence: Oon wondered that I would buy JYSK's C50 yenknife bed.\n",
            "Polarity: 0.0\n",
            "Sentence: Often hears that a good bed gets but from four tons so I do not really know.\n",
            "Polarity: 0.3\n",
            "Sentence: In the store it tried and liked, a suitable robust just what I want.\n",
            "Polarity: 0.575\n",
            "Sentence: Tulis to pay for it in 825, normal 2100.\n",
            "Polarity: 0.15\n",
            "Sentence: So oo amis and selfish son.\n",
            "Polarity: -0.5\n",
            "Sentence: In their own field, there is a single-looking son.\n",
            "Polarity: 0.6\n",
            "Sentence: I never tucked with it and I don't know how I could start a conversation.\n",
            "Polarity: 0.0\n",
            "Sentence: Of course, when the same field is being studied so I could use it as the subject MUT do not know how.\n",
            "Polarity: -0.08333333333333333\n",
            "Suomi, translated version\n",
            "Positives: 29\n",
            "Negatives: 18\n",
            "Portion Positive: 0.6170212765957447\n",
            "Average Polarity: 0.06395275923921757\n",
            "End marker\n"
          ]
        }
      ]
    }
  ]
}