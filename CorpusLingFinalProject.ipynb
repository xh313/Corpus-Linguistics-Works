{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CorpusLingFinalProject.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPFQqlwZ9/NNCeRx10M7oAh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xh313/Corpus-Linguistics-Works/blob/main/CorpusLingFinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Planning"
      ],
      "metadata": {
        "id": "8rF6plh7ycKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspiration:\n",
        "![meme](https://user-images.githubusercontent.com/77285010/161146641-d099beed-56d7-40ef-8ae4-0083fe473d6f.png)\n",
        "\n",
        "(source: https://www.reddit.com/r/Suomi/comments/tqx5ac/mielipide_suomesta_eri_alaredditeiss%C3%A4/)\n",
        "\n",
        "When I was learning Finnish through browsing the Finnish language community on reddit r/suomi (meaning Finland in Finnish), I saw this meme which makes a contrast between the English language subreddit on Finland and the native language one. Obviously, the contrast between the sentiment on the place of the natives and the 'tourists' is pretty hilarious. I am thus intrigued to see what the differences are through analysing the corpora!\n"
      ],
      "metadata": {
        "id": "k596vr7xuvRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Idea\n",
        "\n",
        "- Scrap the English and native language subreddits of different places in the world and compare sentiments.\n",
        "- Assume that the English corpora represent the discussions within 'international community', while the native corpora represent the 'local community'.\n",
        "- Compare and contrast:\n",
        "  - English vs Native sentiments\n",
        "  - How the contrast is different for different countries / places around the world."
      ],
      "metadata": {
        "id": "a8wNNVUVv5KU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methods\n",
        "- Scrap the subreddits\n",
        "- Translation: first compare if the sentiment of the translated corpora is similar enough to the original corpora by sampling a few languages\n",
        "- If translation does not strip the sentiment too much, the translated corpora would be used to do comparative analysis."
      ],
      "metadata": {
        "id": "KQWpV0-yxpNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Channel lists:\n",
        "E = English, N = Native\n",
        "\n",
        "- Poland:\n",
        "  - E: https://www.reddit.com/r/poland/\n",
        "  - N: https://www.reddit.com/r/Polska/\n",
        "\n",
        "- Finland:\n",
        "  - E: https://www.reddit.com/r/Finland/\n",
        "  - N: https://www.reddit.com/r/Suomi/\n",
        "\n",
        "- Brazil:\n",
        "  - E: https://www.reddit.com/r/Brazil/\n",
        "  - N: https://www.reddit.com/r/brasil/\n",
        "\n",
        "- Turkey:\n",
        "  - E: https://www.reddit.com/r/Turkey/\n",
        "  - N: https://www.reddit.com/r/Turkiye/"
      ],
      "metadata": {
        "id": "62IIcmhs2jWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Channel lists\n",
        "test_srs = ['poland', 'Polska', \n",
        "       'Finland', 'Suomi', \n",
        "       'Brazil', 'brasil'\n",
        "       ]\n",
        "\n",
        "srs = ['poland', 'Polska', \n",
        "       'Finland', 'Suomi', \n",
        "       'Brazil', 'brasil',\n",
        "       'Turkey', 'Turkiye',\n",
        "       #'China', 'China_irl',\n",
        "       'japan', 'ja',\n",
        "\n",
        "       ]"
      ],
      "metadata": {
        "id": "ImvYf0MSCW4B"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Issues / Limitations"
      ],
      "metadata": {
        "id": "KPVf9O__yFSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To-be-resolved list\n",
        "\n",
        "1. Some native language forums have bits and pieces of English mixed in, or just reddit codes and stuff in English.\n",
        "2. How many hot posts should I extract per forum?"
      ],
      "metadata": {
        "id": "layPPId-yL-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unresolved limitations\n",
        "\n",
        "1. Size of subreddits vary a lot\n",
        "2. Naturally the topic focus would be subtly different in different subreddits, which ones are comparable?"
      ],
      "metadata": {
        "id": "kyjzv9jWyUY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codes"
      ],
      "metadata": {
        "id": "FtyDqvwsyjB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations / Mounting / Importing"
      ],
      "metadata": {
        "id": "xdUWuwNY41vg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installations"
      ],
      "metadata": {
        "id": "sKicHvp15EPy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiUo8q5Iupna",
        "outputId": "abb88c25-12e7-4246-d8c2-0b0bfeb95730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.5.0-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 174 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 176 kB 14.7 MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.18\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting websocket-client>=0.54.0\n",
            "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting prawcore<3,>=2.1\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n",
            "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
            "Successfully installed praw-7.5.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install praw"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing"
      ],
      "metadata": {
        "id": "Z_N54e_g5HBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "\n",
        "reddit = praw.Reddit(client_id=\"R7keGb8iIXeFuLo-CZOs7g\",\n",
        "                     client_secret=\"ieKEfzD3VvRdmg-HGR0yUt3A6jD_Lw\",\n",
        "                     user_agent=\"scrapata\",\n",
        "                     username='hxhthewebscraper',\n",
        "                     password='Puhun_suomea4')\n",
        "\n",
        "print(reddit.user.me())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpfhZPFQ5J7S",
        "outputId": "f6cfc6a2-913b-4087-daae-cee51b71bf8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hxhthewebscraper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "SKnFVzyUHkC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting"
      ],
      "metadata": {
        "id": "NPT--q3OHbZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iopko5FHHdy8",
        "outputId": "bd55e351-c28b-460e-fb60-c99582e50377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping"
      ],
      "metadata": {
        "id": "aGgGlanM5Rli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subr_to_list(subreddit, number):\n",
        "  \"\"\"\n",
        "  Takes a subreddit name and scraps posts into a list.\n",
        "  Returns the list.\n",
        "  \"\"\"\n",
        "  subr_list = list() #key is body, value is judgement\n",
        "\n",
        "  # get 10 hot posts from the r/Suomi subreddit\n",
        "  hot_posts = reddit.subreddit(subreddit).hot(limit=number) #this is pulling out the hot posts\n",
        "  for post in hot_posts:  \n",
        "      submission = reddit.submission(id=post.id)\n",
        "      subr_list.append(submission.selftext)\n",
        "      #linked_post = reddit.submission(url='https://www.reddit/com/'+post.url)\n",
        "      #for top_level_comment in submission.comments:\n",
        "        #print(top_level_comment.body)\n",
        "        #subr_dict[submission.selftext] = top_level_comment.body\n",
        "\n",
        "  return subr_list"
      ],
      "metadata": {
        "id": "r3n47ufx5QNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_to_txt(postlist, filename, ordinal='1'):\n",
        "  \"\"\"\n",
        "  Takes a list of processed subreddit posts and output a .txt file.\n",
        "  The ordinal in the end is added to sort the files properly.\n",
        "  \"\"\"\n",
        "  outfile = open('/content/drive/My Drive/LGCS124/FinalProject/TestCorpora/' + str(int(ordinal)) + '_' + filename + '.txt', 'w')\n",
        "\n",
        "  for post in postlist:\n",
        "    outfile.write(post)\n",
        "    outfile.write('\\n')\n",
        "\n",
        "  outfile.close()"
      ],
      "metadata": {
        "id": "TtTzIo3i8_I6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester: [DON'T RERUN THIS IT TAKES AGES!!!]\n",
        "\n",
        "finland_e = subr_to_list('Suomi', 100)"
      ],
      "metadata": {
        "id": "LWwphQBu6kKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_to_txt(finland_e, 'finland_e')"
      ],
      "metadata": {
        "id": "pWXKAQwx-wI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The ultimate scraper!"
      ],
      "metadata": {
        "id": "dSzYQ9ebBuhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subr_extract(subreddit, number, ordinal='1'):\n",
        "  \"\"\"\n",
        "  The ultimate extraction. Takes a string that is the name of the subreddit\n",
        "  and the number of posts to be scraped.\n",
        "  The ordinal in the end is added to sort the files properly.\n",
        "  \"\"\"\n",
        "  postlist = subr_to_list(subreddit, number)\n",
        "  list_to_txt(postlist, subreddit, ordinal)"
      ],
      "metadata": {
        "id": "syU0rKkC_vbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester\n",
        "subr_extract('Polska', 10)"
      ],
      "metadata": {
        "id": "O1i0cPXMBSaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tester\n",
        "ord = 1\n",
        "for sr in test_srs:\n",
        "  subr_extract(sr, 10, ord)\n",
        "  ord += 0.5"
      ],
      "metadata": {
        "id": "-Rl5dWY-C1Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function written to scrape the list [srs](https://colab.research.google.com/drive/1Fh5wP9f8Qt47cjE8xgT86ub9eWUZo-iR#scrollTo=ImvYf0MSCW4B&line=3&uniqifier=1)"
      ],
      "metadata": {
        "id": "eRl5cvzIGPjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrap_srs(currentsrs, number):\n",
        "  ord = 1\n",
        "  for sr in currentsrs:\n",
        "    subr_extract(sr, number, ord)\n",
        "    ord += 0.5"
      ],
      "metadata": {
        "id": "-8i-wlBBGrwF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call this to get all files from current srs into the drive\n"
      ],
      "metadata": {
        "id": "8w09pkOeIa5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DON'T RUN UNLESS YOU WANT TO GET THE FILE NOW!\n",
        "scrap_srs(srs, 10)"
      ],
      "metadata": {
        "id": "2G1vILroIiY9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}