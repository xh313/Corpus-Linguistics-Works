{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CorpusLingHW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMGqv8VVwMP+qidvFfN5XP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xh313/Corpus-Linguistics-Works/blob/main/CorpusLingHW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LGCS124 HW3\n",
        "\n",
        "Xuehuai He"
      ],
      "metadata": {
        "id": "2l6kdvOouMgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing, mounting etc.\n",
        "\n",
        "Everything that needs to be imported!"
      ],
      "metadata": {
        "id": "lgoJoQZ-2oAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3-Z47hJuHYe",
        "outputId": "c6f2e483-fbf0-4c03-8fe9-60beb9f0f698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "import string\n",
        "import regex as re\n",
        "from collections import Counter\n",
        "from nltk import ngrams\n",
        "\n",
        "\n",
        "infile1 = open('/content/drive/My Drive/LGCS124/Copy of austen-emma.txt', 'r')\n",
        "austen_emma = infile1.read()\n",
        "infile2 = open('/content/drive/My Drive/LGCS124/Copy of austen-sense.txt', 'r')\n",
        "austen_sense = infile2.read()\n",
        "infile3 = open('/content/drive/My Drive/LGCS124/Copy of austen-persuasion.txt', 'r')\n",
        "austen_persuasion = infile3.read()\n",
        "infile4 = open('/content/drive/My Drive/LGCS124/Copy of bible-kjv.txt', 'r')\n",
        "bible_kjv = infile4.read()\n",
        "infile5 = open('/content/drive/My Drive/LGCS124/Copy of shakespeare.txt', 'r')\n",
        "shakespeare = infile5.read()\n",
        "infile6 = open('/content/drive/My Drive/LGCS124/Copy of whitman-leaves.txt', 'r')\n",
        "whitman_leaves = infile6.read()\n",
        "infile7 = open('/content/drive/My Drive/LGCS124/Copy of independence.txt', 'r')\n",
        "independence = infile7.read()\n",
        "infile8 = open('/content/drive/My Drive/LGCS124/Copy of milton-paradise.txt', 'r')\n",
        "milton_paradise = infile8.read()\n",
        "infile9 = open('/content/drive/My Drive/LGCS124/Copy of hawthorne-gables.txt', 'r')\n",
        "hawthorne_gables = infile9.read()\n",
        "infile10 = open('/content/drive/My Drive/LGCS124/Copy of constitution.txt', 'r')\n",
        "constitution = infile10.read()\n",
        "infile11 = open('/content/drive/My Drive/LGCS124/Copy of blake-songs.txt', 'r')\n",
        "blake_songs = infile11.read()\n",
        "infile12 = open('/content/drive/My Drive/LGCS124/Copy of blake-poems.txt', 'r')\n",
        "blake_poems = infile12.read()\n",
        "infile13 = open('/content/drive/My Drive/LGCS124/Copy of dickens-oliver.txt', 'r')\n",
        "dickens_oliver = infile13.read()\n",
        "infile14 = open('/content/drive/My Drive/LGCS124/Copy of dickens-expectations.txt', 'r')\n",
        "dickens_expectations = infile14.read()\n",
        "infile15 = open('/content/drive/My Drive/LGCS124/Copy of dickens-drood.txt', 'r')\n",
        "dickens_drood = infile15.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. The shortest verse in the Bible is “43:011:035 Jesus wept.” Find another two-word verse.\n",
        "NOTE: We'll consider a \"verse\" to be text starting with a sequence of numbers and colons of the\n",
        "form nn:nnn:nnn at the beginning of a line."
      ],
      "metadata": {
        "id": "pm5I9sdCutRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verses = re.findall(r'[0-9]{2}:[0-9]{3}:[0-9]{3}\\s[a-z|A-Z]+\\s[a-z|A-Z]+\\W\\n', bible_kjv)\n",
        "\n",
        "print(verses[0].strip(), verses[1].strip())  # Stripping the \\n trailing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlCYTfrEv-zt",
        "outputId": "5cc457fb-0387-46c5-980f-3b253d45c7cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43:011:035 Jesus wept. 52:005:016 Rejoice evermore.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. The website regexcrossword.com offers regular expression-based puzzles. What is the solution to Intermediate puzzle #4? (http://regexcrossword.com/challenges/intermediate/puzzles/4)\n",
        "\n",
        "Answer:\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/77285010/154583221-c790aa0a-a7cc-42aa-966c-6fef6d18ff54.png)"
      ],
      "metadata": {
        "id": "B8_RDSq7vzrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. There are 10 uses of the phrase “he or she” in Walt Whitman's Leaves of Grass. Extract just these lines, then (using either regex or string methods) change each “he or she” to “they.” Then look at your new strings. In looking at your output, what new problem have you now introduced? (You don't need to solve the problem, just identify it!)"
      ],
      "metadata": {
        "id": "WNgkY5Dy2YZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whitman_leaves = whitman_leaves.lower()  # Lowercase all\n",
        "splitted = whitman_leaves.split('\\n')  # Split by lines\n",
        "\n",
        "\n",
        "# Removing all lines without he or she\n",
        "containing = []  # Empty list to store the lines with pronouns\n",
        "for i in range(len(splitted)):\n",
        "  res = re.findall(r'\\bhe or she\\b', splitted[i])\n",
        "  if res != []:\n",
        "    containing.append(splitted[i])\n",
        "\n",
        "print(containing)\n",
        "print(len(containing))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS9ejNen2IYo",
        "outputId": "2dff50c2-fad3-4864-8fe4-d249ad9f227e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['  do you suppose you have a right to a good sight, and he or she has', '  whoever accepts me he or she shall be blessed and shall bless me.', '  none may come to the trial till he or she bring courage and health,', \"  when he or she appears materials are overaw'd,\", '  whoever you are! you are he or she for whom the earth is solid and liquid,', '  you are he or she for whom the sun and moon hang in the sky,', '      dissolution, you are he or she who is master or mistress over them,', '  he or she is greatest who contributes the greatest original', '  in exact proportion to what he or she grew from in life, and out of', \"      what he or she did, felt, became, loved, sinn'd, in life.\"]\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing \n",
        "replaced = []\n",
        "\n",
        "for i in range(len(containing)):\n",
        "  replaced.append(re.sub(r'\\bhe or she\\b', 'they', containing[i]))\n",
        "\n",
        "print(replaced)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boowqGuT6yh6",
        "outputId": "bde712ee-1897-49c4-80f7-d0979b5211b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['  do you suppose you have a right to a good sight, and they has', '  whoever accepts me they shall be blessed and shall bless me.', '  none may come to the trial till they bring courage and health,', \"  when they appears materials are overaw'd,\", '  whoever you are! you are they for whom the earth is solid and liquid,', '  you are they for whom the sun and moon hang in the sky,', '      dissolution, you are they who is master or mistress over them,', '  they is greatest who contributes the greatest original', '  in exact proportion to what they grew from in life, and out of', \"      what they did, felt, became, loved, sinn'd, in life.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem: the grammar is now completely messed up and the conjugations and cases are not right!"
      ],
      "metadata": {
        "id": "1FqqYXN29HAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. Look at the numbers in file nums1.txt in the /data/classes/124/homework/hw3 folder. Use re.sub to add commas where appropriate. For example, change \"1000\" to \"1,000\". For extra credit, you might (optionally) see if you can also appropriately handle all the numbers in Rile nums2.txt. For example, \"37373.37373\" should be \"37,372.37373\", while \"0.9872\" remains unchanged."
      ],
      "metadata": {
        "id": "MQgEaG34-Aoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(SKIPPED PROBLEM)"
      ],
      "metadata": {
        "id": "yVKUcKZK-eMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. Extract all phonetic transcriptions from the file. How many different unique transcriptions are there?"
      ],
      "metadata": {
        "id": "PmnGfNAfzOHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing\n",
        "infileepw = open('/content/drive/My Drive/LGCS124/epw.cd', 'r')\n",
        "epw = infileepw.read()\n",
        "\n",
        "# Separating lines\n",
        "sep = epw.split('\\n')\n",
        "\n",
        "# Separating out the content after the last slash\n",
        "def extract_pron(data, delim):\n",
        "  extracted = []\n",
        "  for i in range(len(data)):\n",
        "    extracted.append(data[i].split(delim)[-1])\n",
        "  return extracted\n",
        "\n",
        "pronunciation = extract_pron(sep, '\\\\')  # All pronunciations get extracted!\n",
        "\n",
        "print(pronunciation[:50])  # Printing out the first 50 as an example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK5D8KXqzg8L",
        "outputId": "582f6e75-6514-4030-8a6e-a6ee1d6d2c45"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[@]', '[eI]', '[eI]', '[eI]', '[eI][eI]', '[eI][eI]', '[eI][eIz]', '[&][b@][saI]', '[@][b&k]', '[&][b@][k@s]', '[&][b@][k@][sIz]', '[@][bA:ft]', '[@][bA:ft]', '[@][b&n][d@n]', '[@][b&n][d@n]', '[@][b&n][d@nd]', '[@][b&n][d@nd]', '[@][b&n][d@][nIN]', '[@][b&n][d@n][m@nt]', '[@][b&n][d@nz]', '[@][beIs]', '[@][beIst]', '[@][beIs][m@nt]', '[@][beI][sIz]', '[@][b&S]', '[@][b&St]', '[@][b&][SIz]', '[@][b&][SIN]', '[@][beI][sIN]', '[@][beIt]', '[@][beI][tId]', '[@][beIt][m@nt]', '[@][beIts]', '[@][beI][tIN]', '[&][b@][twO:r*]', '[&][b@][twO:z]', '[&][beI]', '[&][beIz]', '[&][bIs]', '[&][bI][sIz]', '[&][bI]', '[&][bIz]', '[&][b@t]', '[&][b@ts]', '[@][bri:][vjeIt]', '[@][bri:][vjeI][tId]', '[@][bri:][vjeIts]', '[@][bri:][vjeI][tIN]', '[@][bri:][vI][eI][Sn,]', '[@][bri:][vI][eI][Sn,z]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the number of unique pronunciations\n",
        "\n",
        "pron_counter = Counter(pronunciation)\n",
        "\n",
        "print(len(pron_counter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe3Y4iQG6Wrx",
        "outputId": "81b0ac56-d0cc-4df1-e632-b1995f92deac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 87560 unique types of pronunciations!"
      ],
      "metadata": {
        "id": "vr33eb6u6yIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6\n",
        "\n"
      ],
      "metadata": {
        "id": "wTINmRtE8-Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing\n",
        "infilebnc = open('/content/drive/My Drive/LGCS124/A00.xml', 'r')\n",
        "bnc = infilebnc.read()\n",
        "\n",
        "hw = re.findall(r'(hw\\=\\\"[a-z]+\\\")', bnc)\n",
        "\n",
        "lemmas = []\n",
        "for i in range(len(hw)):\n",
        "  lemmas.append(hw[i].split('\\\"')[-2])\n",
        "\n",
        "print(lemmas[:10])  # First 10 lemmas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IMYRViF-Xwc",
        "outputId": "f61414f8-5063-4ed2-df48-f3268106c63f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['factsheet', 'what', 'be', 'aids', 'aids', 'acquire', 'immune', 'deficiency', 'syndrome', 'be']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of unique lemmas\n",
        "\n",
        "lemma_counter = Counter(lemmas)\n",
        "\n",
        "print(len(lemma_counter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGml9DQu_wWe",
        "outputId": "82701467-74fc-4aa6-8d2e-7d8be2c10d0d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 1271 unique lemmas."
      ],
      "metadata": {
        "id": "1J9fU2URAMZB"
      }
    }
  ]
}